{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rRviSxvkVuVX"
   },
   "source": [
    "<h1 style=\"text-align:center;font-size:30px;\" > CNN on CIFAR-10 using Keras </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
    "2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n",
    "3.  You cannot use Dense Layers (also called fully connected layers), or DropOut.\n",
    "4.  You MUST use Image Augmentation Techniques.\n",
    "5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
    "6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
    "7.  You cannot use test images for training the model.\n",
    "8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
    "9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
    "10. You cannot have more than 1 Million parameters in total\n",
    "11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n",
    "12. You can use any optimization algorithm you need. \n",
    "13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "id": "wVIx_KIigxPV",
    "outputId": "0702c113-479b-43d0-fc7c-a073c4d8ccbd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6_Jj4UsVuVc"
   },
   "source": [
    "## 1.0 Loading & splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POMEI-0IO7Ih"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mB7o3zu1g6eT",
    "outputId": "db28ea3f-a95d-45e2-ec33-6ad2bebe6ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoding\n",
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KSDSe5QQuWJ9"
   },
   "source": [
    "#### Prepare pixel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jZ5dDNJRbje"
   },
   "outputs": [],
   "source": [
    "# convert from integers to floats\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize to range 0-1\n",
    "X_train  /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xQ9xnO5OVuVj",
    "outputId": "7279d0f5-1200-4d60-c2aa-f8dd93c8618c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ASI4LV39VuVm",
    "outputId": "5d7604c9-d19c-4c43-831a-d040f35dad63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NyUho54tVuVq"
   },
   "source": [
    "## 2.0 Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8qnaZ1GVuWI"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 300\n",
    "l = 6\n",
    "num_filter = 35\n",
    "compression = 1.0\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    Conv = layers.Conv2D(num_classes, (1,1),activation='softmax', use_bias=False ,padding='same')(AvgPooling)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv)\n",
    "    output = layers.Flatten()(avg)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "izxFE9ElVuVv"
   },
   "source": [
    "## 3.0 Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rJOzrQNdVuVx"
   },
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/image-augmentation-deep-learning-keras/\n",
    "#https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# create data generator\n",
    "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True,\n",
    "                             rotation_range=50,fill_mode='nearest',zoom_range=0.10)\n",
    "# prepare iterator\n",
    "it_train = datagen.flow(X_train,y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "px0GC-wDVuV7"
   },
   "source": [
    "## 4.0 Architecture & compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1kFh7pdxhNtT",
    "outputId": "0ceff86d-4874-43ec-cf08-749883436e09",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 32, 32, 35)   945         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 32, 32, 35)   140         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 32, 32, 35)   0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 32, 32, 35)   11025       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 32, 32, 35)   0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 32, 32, 70)   0           conv2d_58[0][0]                  \n",
      "                                                                 dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 32, 32, 70)   280         concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 32, 32, 70)   0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 32, 32, 35)   22050       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 32, 32, 35)   0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 32, 32, 105)  0           concatenate_48[0][0]             \n",
      "                                                                 dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 32, 105)  420         concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 32, 32, 105)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 32, 32, 35)   33075       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 32, 32, 35)   0           conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 32, 32, 140)  0           concatenate_49[0][0]             \n",
      "                                                                 dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 32, 32, 140)  560         concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 32, 32, 140)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 32, 32, 35)   44100       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 32, 32, 35)   0           conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 32, 32, 175)  0           concatenate_50[0][0]             \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 32, 32, 175)  700         concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 32, 32, 175)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 32, 32, 35)   55125       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 32, 32, 35)   0           conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 32, 32, 210)  0           concatenate_51[0][0]             \n",
      "                                                                 dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 32, 32, 210)  840         concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 32, 32, 210)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 32, 32, 35)   66150       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 32, 32, 35)   0           conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, 32, 32, 245)  0           concatenate_52[0][0]             \n",
      "                                                                 dropout_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 32, 32, 245)  980         concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 32, 32, 245)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 32, 32, 35)   8575        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 32, 32, 35)   0           conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 16, 16, 35)   0           dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16, 16, 35)   140         average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 16, 16, 35)   0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 16, 35)   11025       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 16, 16, 35)   0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_54 (Concatenate)    (None, 16, 16, 70)   0           average_pooling2d_10[0][0]       \n",
      "                                                                 dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 16, 16, 70)   280         concatenate_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 16, 16, 70)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 35)   22050       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 16, 16, 35)   0           conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 16, 16, 105)  0           concatenate_54[0][0]             \n",
      "                                                                 dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 16, 16, 105)  420         concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 16, 16, 105)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 16, 16, 35)   33075       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 16, 16, 35)   0           conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 16, 16, 140)  0           concatenate_55[0][0]             \n",
      "                                                                 dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 16, 16, 140)  560         concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 16, 16, 140)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 16, 16, 35)   44100       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, 16, 16, 35)   0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 16, 16, 175)  0           concatenate_56[0][0]             \n",
      "                                                                 dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 16, 16, 175)  700         concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 16, 16, 175)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 16, 16, 35)   55125       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 16, 16, 35)   0           conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 16, 16, 210)  0           concatenate_57[0][0]             \n",
      "                                                                 dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 16, 16, 210)  840         concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 16, 16, 210)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 16, 16, 35)   66150       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 16, 16, 35)   0           conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 16, 16, 245)  0           concatenate_58[0][0]             \n",
      "                                                                 dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 16, 16, 245)  980         concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 16, 16, 245)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 16, 16, 35)   8575        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 16, 16, 35)   0           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 8, 8, 35)     0           dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 8, 8, 35)     140         average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 8, 8, 35)     0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 8, 8, 35)     11025       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 8, 8, 35)     0           conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 8, 8, 70)     0           average_pooling2d_11[0][0]       \n",
      "                                                                 dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 8, 70)     280         concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 8, 8, 70)     0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 8, 8, 35)     22050       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 8, 8, 35)     0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 8, 8, 105)    0           concatenate_60[0][0]             \n",
      "                                                                 dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 105)    420         concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 8, 105)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 8, 8, 35)     33075       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 8, 8, 35)     0           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 8, 8, 140)    0           concatenate_61[0][0]             \n",
      "                                                                 dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 8, 8, 140)    560         concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 8, 8, 140)    0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 8, 35)     44100       activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 8, 8, 35)     0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 8, 8, 175)    0           concatenate_62[0][0]             \n",
      "                                                                 dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 8, 8, 175)    700         concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 8, 8, 175)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 8, 35)     55125       activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 8, 8, 35)     0           conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_64 (Concatenate)    (None, 8, 8, 210)    0           concatenate_63[0][0]             \n",
      "                                                                 dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 8, 8, 210)    840         concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 8, 8, 210)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 8, 35)     66150       activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 8, 8, 35)     0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_65 (Concatenate)    (None, 8, 8, 245)    0           concatenate_64[0][0]             \n",
      "                                                                 dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 8, 8, 245)    980         concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 8, 8, 245)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 35)     8575        activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 8, 8, 35)     0           conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 4, 4, 35)     0           dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 4, 4, 35)     140         average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 4, 4, 35)     0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 4, 4, 35)     11025       activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 4, 4, 35)     0           conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, 4, 4, 70)     0           average_pooling2d_12[0][0]       \n",
      "                                                                 dropout_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 4, 4, 70)     280         concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 4, 4, 70)     0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 4, 4, 35)     22050       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 4, 4, 35)     0           conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 4, 4, 105)    0           concatenate_66[0][0]             \n",
      "                                                                 dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 4, 4, 105)    420         concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 4, 4, 105)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 4, 4, 35)     33075       activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 4, 4, 35)     0           conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, 4, 4, 140)    0           concatenate_67[0][0]             \n",
      "                                                                 dropout_77[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 4, 4, 140)    560         concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 4, 4, 140)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 4, 4, 35)     44100       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, 4, 4, 35)     0           conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, 4, 4, 175)    0           concatenate_68[0][0]             \n",
      "                                                                 dropout_78[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 4, 4, 175)    700         concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 4, 4, 175)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 4, 4, 35)     55125       activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 4, 4, 35)     0           conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_70 (Concatenate)    (None, 4, 4, 210)    0           concatenate_69[0][0]             \n",
      "                                                                 dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 4, 4, 210)    840         concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 4, 4, 210)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 4, 4, 35)     66150       activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 4, 4, 35)     0           conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_71 (Concatenate)    (None, 4, 4, 245)    0           concatenate_70[0][0]             \n",
      "                                                                 dropout_80[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 4, 4, 245)    980         concatenate_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 4, 4, 245)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_13 (AveragePo (None, 2, 2, 245)    0           activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 2, 2, 10)     2450        average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 1, 1, 10)     0           conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10)           0           average_pooling2d_14[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 970,900\n",
      "Trainable params: 963,060\n",
      "Non-trainable params: 7,840\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ox609ujh_gOv"
   },
   "outputs": [],
   "source": [
    "# Loading the checkpoints if required\n",
    "model.load_weights(\"/content/model-ep107-val_loss0.327.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wh-3N5odugt1"
   },
   "source": [
    "## 5.0 Checkpointing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlX2mMpTufxy"
   },
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filepath = 'model-ep{epoch:03d}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoints = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "train_results = CSVLogger('train_results.log') #storing the training results in a pandas dataframe\n",
    "callbacks_list = [checkpoints, train_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzOpEVZO7q4X"
   },
   "source": [
    "## 6.0 Fitting the model in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UQaiELLVVuWK",
    "outputId": "14d552f6-a9dd-489e-ad72-39fe50ff43b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2734 - acc: 0.9057Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 4s 395us/sample - loss: 0.3820 - acc: 0.8761\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42766, saving model to model-ep001-val_loss0.428.h5\n",
      "781/781 [==============================] - 86s 110ms/step - loss: 0.2736 - acc: 0.9056 - val_loss: 0.4277 - val_acc: 0.8761\n",
      "Epoch 2/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9031Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 263us/sample - loss: 0.2649 - acc: 0.8753\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.42766\n",
      "781/781 [==============================] - 55s 70ms/step - loss: 0.2743 - acc: 0.9031 - val_loss: 0.4337 - val_acc: 0.8753\n",
      "Epoch 3/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9045Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 263us/sample - loss: 0.3072 - acc: 0.8814\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.42766 to 0.41082, saving model to model-ep003-val_loss0.411.h5\n",
      "781/781 [==============================] - 55s 70ms/step - loss: 0.2690 - acc: 0.9045 - val_loss: 0.4108 - val_acc: 0.8814\n",
      "Epoch 4/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2777 - acc: 0.9038Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 263us/sample - loss: 0.3329 - acc: 0.8760\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.41082\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2779 - acc: 0.9037 - val_loss: 0.4495 - val_acc: 0.8760\n",
      "Epoch 5/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.9080Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2192 - acc: 0.8958\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.41082 to 0.34551, saving model to model-ep005-val_loss0.346.h5\n",
      "781/781 [==============================] - 54s 70ms/step - loss: 0.2669 - acc: 0.9080 - val_loss: 0.3455 - val_acc: 0.8958\n",
      "Epoch 6/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2677 - acc: 0.9056Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.4427 - acc: 0.8722\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2677 - acc: 0.9056 - val_loss: 0.4710 - val_acc: 0.8722\n",
      "Epoch 7/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2681 - acc: 0.9065Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2110 - acc: 0.8905\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2682 - acc: 0.9065 - val_loss: 0.3866 - val_acc: 0.8905\n",
      "Epoch 8/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2633 - acc: 0.9073Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.2458 - acc: 0.8894\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2633 - acc: 0.9072 - val_loss: 0.3720 - val_acc: 0.8894\n",
      "Epoch 9/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9055Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.3538 - acc: 0.8922\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2726 - acc: 0.9055 - val_loss: 0.3690 - val_acc: 0.8922\n",
      "Epoch 10/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.9083Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.4118 - acc: 0.8785\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2617 - acc: 0.9084 - val_loss: 0.4332 - val_acc: 0.8785\n",
      "Epoch 11/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.9085Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.4115 - acc: 0.8886\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2660 - acc: 0.9086 - val_loss: 0.3884 - val_acc: 0.8886\n",
      "Epoch 12/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2581 - acc: 0.9083Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2369 - acc: 0.8940\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2582 - acc: 0.9083 - val_loss: 0.3471 - val_acc: 0.8940\n",
      "Epoch 13/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2641 - acc: 0.9091Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.4625 - acc: 0.8719\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2642 - acc: 0.9091 - val_loss: 0.4421 - val_acc: 0.8719\n",
      "Epoch 14/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9077Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.3306 - acc: 0.8838\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2582 - acc: 0.9077 - val_loss: 0.4147 - val_acc: 0.8838\n",
      "Epoch 15/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9101Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 263us/sample - loss: 0.3943 - acc: 0.8837\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2598 - acc: 0.9101 - val_loss: 0.3929 - val_acc: 0.8837\n",
      "Epoch 16/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.9102Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.4344 - acc: 0.8665\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2574 - acc: 0.9101 - val_loss: 0.4637 - val_acc: 0.8665\n",
      "Epoch 17/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.9089Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.3105 - acc: 0.8685\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2604 - acc: 0.9088 - val_loss: 0.4646 - val_acc: 0.8685\n",
      "Epoch 18/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9109Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 263us/sample - loss: 0.3600 - acc: 0.8875\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2568 - acc: 0.9109 - val_loss: 0.3745 - val_acc: 0.8875\n",
      "Epoch 19/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2576 - acc: 0.9093Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.3138 - acc: 0.8785\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2577 - acc: 0.9093 - val_loss: 0.4614 - val_acc: 0.8785\n",
      "Epoch 20/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9126Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.3178 - acc: 0.8716\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2516 - acc: 0.9125 - val_loss: 0.4768 - val_acc: 0.8716\n",
      "Epoch 21/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9084Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.5202 - acc: 0.8863\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2606 - acc: 0.9083 - val_loss: 0.4098 - val_acc: 0.8863\n",
      "Epoch 22/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2506 - acc: 0.9129Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.4248 - acc: 0.8760\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2506 - acc: 0.9129 - val_loss: 0.4550 - val_acc: 0.8760\n",
      "Epoch 23/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9108Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.3748 - acc: 0.8917\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2551 - acc: 0.9108 - val_loss: 0.3828 - val_acc: 0.8917\n",
      "Epoch 24/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9128Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 263us/sample - loss: 0.3609 - acc: 0.8798\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2491 - acc: 0.9128 - val_loss: 0.4236 - val_acc: 0.8798\n",
      "Epoch 25/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9129Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.3563 - acc: 0.8874\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2489 - acc: 0.9130 - val_loss: 0.3932 - val_acc: 0.8874\n",
      "Epoch 26/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9120Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.2423 - acc: 0.8922\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2505 - acc: 0.9120 - val_loss: 0.3787 - val_acc: 0.8922\n",
      "Epoch 27/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9142Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2663 - acc: 0.8857\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.34551\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2474 - acc: 0.9142 - val_loss: 0.4097 - val_acc: 0.8857\n",
      "Epoch 28/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2504 - acc: 0.9125Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2190 - acc: 0.8999\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.34551 to 0.32856, saving model to model-ep028-val_loss0.329.h5\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2505 - acc: 0.9125 - val_loss: 0.3286 - val_acc: 0.8999\n",
      "Epoch 29/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9128Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.3879 - acc: 0.8646\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.32856\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2512 - acc: 0.9128 - val_loss: 0.5083 - val_acc: 0.8646\n",
      "Epoch 30/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9139Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 263us/sample - loss: 0.3100 - acc: 0.8965\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.32856\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2473 - acc: 0.9138 - val_loss: 0.3585 - val_acc: 0.8965\n",
      "Epoch 31/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2420 - acc: 0.9154Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2362 - acc: 0.8903\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.32856\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2420 - acc: 0.9154 - val_loss: 0.3906 - val_acc: 0.8903\n",
      "Epoch 32/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.9131Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.1797 - acc: 0.9035\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.32856\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2478 - acc: 0.9131 - val_loss: 0.3336 - val_acc: 0.9035\n",
      "Epoch 33/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9141Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2497 - acc: 0.8940\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.32856\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2450 - acc: 0.9141 - val_loss: 0.3691 - val_acc: 0.8940\n",
      "Epoch 34/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9151Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2636 - acc: 0.8849\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.32856\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2422 - acc: 0.9151 - val_loss: 0.4112 - val_acc: 0.8849\n",
      "Epoch 35/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9128Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.3300 - acc: 0.8890\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.32856\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2480 - acc: 0.9128 - val_loss: 0.3781 - val_acc: 0.8890\n",
      "Epoch 36/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9165Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 263us/sample - loss: 0.3954 - acc: 0.8806\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.32856\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2409 - acc: 0.9165 - val_loss: 0.4239 - val_acc: 0.8806\n",
      "Epoch 37/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2414 - acc: 0.9164Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2428 - acc: 0.9008\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.32856\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2413 - acc: 0.9164 - val_loss: 0.3354 - val_acc: 0.9008\n",
      "Epoch 38/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2385 - acc: 0.9167Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 255us/sample - loss: 0.3103 - acc: 0.9013\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.32856 to 0.32717, saving model to model-ep038-val_loss0.327.h5\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2383 - acc: 0.9168 - val_loss: 0.3272 - val_acc: 0.9013\n",
      "Epoch 39/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9124Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2949 - acc: 0.8867\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2452 - acc: 0.9124 - val_loss: 0.3812 - val_acc: 0.8867\n",
      "Epoch 40/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9158Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2940 - acc: 0.8942\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2410 - acc: 0.9158 - val_loss: 0.3623 - val_acc: 0.8942\n",
      "Epoch 41/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9180Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.3376 - acc: 0.8929\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2350 - acc: 0.9181 - val_loss: 0.3866 - val_acc: 0.8929\n",
      "Epoch 42/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9149Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 256us/sample - loss: 0.4359 - acc: 0.8913\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2390 - acc: 0.9149 - val_loss: 0.3930 - val_acc: 0.8913\n",
      "Epoch 43/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9163Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2739 - acc: 0.8928\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2353 - acc: 0.9163 - val_loss: 0.3506 - val_acc: 0.8928\n",
      "Epoch 44/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9158Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 256us/sample - loss: 0.2390 - acc: 0.9026\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2384 - acc: 0.9159 - val_loss: 0.3485 - val_acc: 0.9026\n",
      "Epoch 45/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9171Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2971 - acc: 0.8749\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2366 - acc: 0.9171 - val_loss: 0.4594 - val_acc: 0.8749\n",
      "Epoch 46/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9191Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.3397 - acc: 0.8770\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 67ms/step - loss: 0.2327 - acc: 0.9191 - val_loss: 0.4508 - val_acc: 0.8770\n",
      "Epoch 47/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9188Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.4164 - acc: 0.8772\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2361 - acc: 0.9188 - val_loss: 0.4599 - val_acc: 0.8772\n",
      "Epoch 48/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9168Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.3615 - acc: 0.8801\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 67ms/step - loss: 0.2358 - acc: 0.9168 - val_loss: 0.4531 - val_acc: 0.8801\n",
      "Epoch 49/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9198Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.3567 - acc: 0.8784\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2284 - acc: 0.9199 - val_loss: 0.4373 - val_acc: 0.8784\n",
      "Epoch 50/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9193Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.3708 - acc: 0.8808\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2321 - acc: 0.9193 - val_loss: 0.4475 - val_acc: 0.8808\n",
      "Epoch 51/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9181Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.3015 - acc: 0.8928\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2292 - acc: 0.9180 - val_loss: 0.4000 - val_acc: 0.8928\n",
      "Epoch 52/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9206Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2776 - acc: 0.8911\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 67ms/step - loss: 0.2287 - acc: 0.9207 - val_loss: 0.3843 - val_acc: 0.8911\n",
      "Epoch 53/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9197Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.3564 - acc: 0.8909\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 67ms/step - loss: 0.2281 - acc: 0.9197 - val_loss: 0.4023 - val_acc: 0.8909\n",
      "Epoch 54/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9180Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.3945 - acc: 0.8790\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 52s 67ms/step - loss: 0.2352 - acc: 0.9179 - val_loss: 0.4450 - val_acc: 0.8790\n",
      "Epoch 55/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9184Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.4613 - acc: 0.8888\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2305 - acc: 0.9183 - val_loss: 0.4142 - val_acc: 0.8888\n",
      "Epoch 56/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9218Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2861 - acc: 0.8910\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 67ms/step - loss: 0.2260 - acc: 0.9218 - val_loss: 0.4130 - val_acc: 0.8910\n",
      "Epoch 57/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9208Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2810 - acc: 0.8977\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 67ms/step - loss: 0.2280 - acc: 0.9207 - val_loss: 0.3693 - val_acc: 0.8977\n",
      "Epoch 58/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9197Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.3040 - acc: 0.8869\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 67ms/step - loss: 0.2267 - acc: 0.9196 - val_loss: 0.4124 - val_acc: 0.8869\n",
      "Epoch 59/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9202Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2731 - acc: 0.8969\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 67ms/step - loss: 0.2261 - acc: 0.9202 - val_loss: 0.3672 - val_acc: 0.8969\n",
      "Epoch 60/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9192Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2259 - acc: 0.8957\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 67ms/step - loss: 0.2289 - acc: 0.9193 - val_loss: 0.3606 - val_acc: 0.8957\n",
      "Epoch 61/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9194Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 254us/sample - loss: 0.3564 - acc: 0.8891\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2264 - acc: 0.9194 - val_loss: 0.4121 - val_acc: 0.8891\n",
      "Epoch 62/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9195Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.2854 - acc: 0.8826\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2306 - acc: 0.9195 - val_loss: 0.4290 - val_acc: 0.8826\n",
      "Epoch 63/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2182 - acc: 0.9225Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 256us/sample - loss: 0.3623 - acc: 0.8820\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 67ms/step - loss: 0.2182 - acc: 0.9225 - val_loss: 0.4227 - val_acc: 0.8820\n",
      "Epoch 64/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9210Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.4697 - acc: 0.8856\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2253 - acc: 0.9210 - val_loss: 0.4246 - val_acc: 0.8856\n",
      "Epoch 65/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9203Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2540 - acc: 0.8908\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2236 - acc: 0.9202 - val_loss: 0.3844 - val_acc: 0.8908\n",
      "Epoch 66/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9218Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2387 - acc: 0.8873\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2210 - acc: 0.9218 - val_loss: 0.4161 - val_acc: 0.8873\n",
      "Epoch 67/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9238Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2804 - acc: 0.8873\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2161 - acc: 0.9238 - val_loss: 0.4128 - val_acc: 0.8873\n",
      "Epoch 68/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9210Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2918 - acc: 0.8868\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2221 - acc: 0.9211 - val_loss: 0.4204 - val_acc: 0.8868\n",
      "Epoch 69/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9220Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2481 - acc: 0.8952\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2218 - acc: 0.9220 - val_loss: 0.3839 - val_acc: 0.8952\n",
      "Epoch 70/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9247Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2360 - acc: 0.8966\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2179 - acc: 0.9248 - val_loss: 0.3704 - val_acc: 0.8966\n",
      "Epoch 71/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9228Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.3484 - acc: 0.8852\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2218 - acc: 0.9228 - val_loss: 0.4280 - val_acc: 0.8852\n",
      "Epoch 72/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9256Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2952 - acc: 0.8877\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2146 - acc: 0.9255 - val_loss: 0.4043 - val_acc: 0.8877\n",
      "Epoch 73/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9233Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 256us/sample - loss: 0.2521 - acc: 0.9018\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2211 - acc: 0.9233 - val_loss: 0.3529 - val_acc: 0.9018\n",
      "Epoch 74/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9239Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.3302 - acc: 0.8807\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2161 - acc: 0.9239 - val_loss: 0.4379 - val_acc: 0.8807\n",
      "Epoch 75/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9235Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2311 - acc: 0.8918\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2194 - acc: 0.9235 - val_loss: 0.3983 - val_acc: 0.8918\n",
      "Epoch 76/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9240Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.3665 - acc: 0.8886\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2146 - acc: 0.9240 - val_loss: 0.4074 - val_acc: 0.8886\n",
      "Epoch 77/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9239Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.3032 - acc: 0.8950\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2169 - acc: 0.9238 - val_loss: 0.3821 - val_acc: 0.8950\n",
      "Epoch 78/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9256Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.3343 - acc: 0.8939\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2126 - acc: 0.9257 - val_loss: 0.3878 - val_acc: 0.8939\n",
      "Epoch 79/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9247Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.3241 - acc: 0.8968\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2137 - acc: 0.9247 - val_loss: 0.3632 - val_acc: 0.8968\n",
      "Epoch 80/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9246Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2262 - acc: 0.8980\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2117 - acc: 0.9246 - val_loss: 0.3853 - val_acc: 0.8980\n",
      "Epoch 81/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9240Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2460 - acc: 0.9019\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2162 - acc: 0.9241 - val_loss: 0.3556 - val_acc: 0.9019\n",
      "Epoch 82/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9247Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.3220 - acc: 0.8918\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2124 - acc: 0.9247 - val_loss: 0.4054 - val_acc: 0.8918\n",
      "Epoch 83/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9250Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2417 - acc: 0.8909\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2116 - acc: 0.9251 - val_loss: 0.3942 - val_acc: 0.8909\n",
      "Epoch 84/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9250Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2805 - acc: 0.8728\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2163 - acc: 0.9250 - val_loss: 0.4736 - val_acc: 0.8728\n",
      "Epoch 85/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9240Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.3177 - acc: 0.8896\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2154 - acc: 0.9240 - val_loss: 0.3966 - val_acc: 0.8896\n",
      "Epoch 86/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9241Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.4045 - acc: 0.8745\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2148 - acc: 0.9241 - val_loss: 0.4704 - val_acc: 0.8745\n",
      "Epoch 87/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9244Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2634 - acc: 0.8993\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2116 - acc: 0.9244 - val_loss: 0.3664 - val_acc: 0.8993\n",
      "Epoch 88/190\n",
      "779/781 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9248Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2377 - acc: 0.8904\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2150 - acc: 0.9247 - val_loss: 0.3992 - val_acc: 0.8904\n",
      "Epoch 89/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9281Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2887 - acc: 0.8821\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2080 - acc: 0.9281 - val_loss: 0.4229 - val_acc: 0.8821\n",
      "Epoch 90/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9255Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.2617 - acc: 0.8830\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2114 - acc: 0.9255 - val_loss: 0.4615 - val_acc: 0.8830\n",
      "Epoch 91/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9259Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.4108 - acc: 0.8892\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2146 - acc: 0.9259 - val_loss: 0.4259 - val_acc: 0.8892\n",
      "Epoch 92/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9269Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2996 - acc: 0.9014\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2083 - acc: 0.9269 - val_loss: 0.3607 - val_acc: 0.9014\n",
      "Epoch 93/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9282Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 254us/sample - loss: 0.2577 - acc: 0.9003\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2066 - acc: 0.9282 - val_loss: 0.3567 - val_acc: 0.9003\n",
      "Epoch 94/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9271Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2488 - acc: 0.8843\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2076 - acc: 0.9272 - val_loss: 0.4376 - val_acc: 0.8843\n",
      "Epoch 95/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9277Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.4653 - acc: 0.8851\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2070 - acc: 0.9278 - val_loss: 0.4394 - val_acc: 0.8851\n",
      "Epoch 96/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9281Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2388 - acc: 0.9027\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2046 - acc: 0.9281 - val_loss: 0.3552 - val_acc: 0.9027\n",
      "Epoch 97/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9269Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2444 - acc: 0.9024\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2100 - acc: 0.9269 - val_loss: 0.3567 - val_acc: 0.9024\n",
      "Epoch 98/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9265Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2783 - acc: 0.8981\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2082 - acc: 0.9264 - val_loss: 0.3626 - val_acc: 0.8981\n",
      "Epoch 99/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9277Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2453 - acc: 0.9004\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2082 - acc: 0.9276 - val_loss: 0.3591 - val_acc: 0.9004\n",
      "Epoch 100/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9284Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2565 - acc: 0.9053\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2044 - acc: 0.9284 - val_loss: 0.3360 - val_acc: 0.9053\n",
      "Epoch 101/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9264Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2645 - acc: 0.8966\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2055 - acc: 0.9264 - val_loss: 0.3706 - val_acc: 0.8966\n",
      "Epoch 102/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9280Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2692 - acc: 0.9031\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2028 - acc: 0.9280 - val_loss: 0.3468 - val_acc: 0.9031\n",
      "Epoch 103/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9281Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2951 - acc: 0.8864\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2036 - acc: 0.9281 - val_loss: 0.4190 - val_acc: 0.8864\n",
      "Epoch 104/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9267Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2374 - acc: 0.9017\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2062 - acc: 0.9267 - val_loss: 0.3624 - val_acc: 0.9017\n",
      "Epoch 105/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9288Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.1975 - acc: 0.9021\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2029 - acc: 0.9288 - val_loss: 0.3644 - val_acc: 0.9021\n",
      "Epoch 106/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9292Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2248 - acc: 0.8987\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1994 - acc: 0.9292 - val_loss: 0.3809 - val_acc: 0.8987\n",
      "Epoch 107/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9305Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2957 - acc: 0.8976\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2007 - acc: 0.9306 - val_loss: 0.3835 - val_acc: 0.8976\n",
      "Epoch 108/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9287Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2348 - acc: 0.9011\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2014 - acc: 0.9286 - val_loss: 0.3757 - val_acc: 0.9011\n",
      "Epoch 109/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9290Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2727 - acc: 0.8933\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2001 - acc: 0.9289 - val_loss: 0.4055 - val_acc: 0.8933\n",
      "Epoch 110/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9294Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 256us/sample - loss: 0.2492 - acc: 0.8946\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2005 - acc: 0.9294 - val_loss: 0.3852 - val_acc: 0.8946\n",
      "Epoch 111/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9331Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.3129 - acc: 0.8991\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1932 - acc: 0.9331 - val_loss: 0.3985 - val_acc: 0.8991\n",
      "Epoch 112/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2051 - acc: 0.9279Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.3518 - acc: 0.8952\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2049 - acc: 0.9279 - val_loss: 0.3887 - val_acc: 0.8952\n",
      "Epoch 113/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1998 - acc: 0.9298Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2904 - acc: 0.8915\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1997 - acc: 0.9298 - val_loss: 0.4188 - val_acc: 0.8915\n",
      "Epoch 114/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9285Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.3203 - acc: 0.8849\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.2014 - acc: 0.9284 - val_loss: 0.4471 - val_acc: 0.8849\n",
      "Epoch 115/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9299Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2351 - acc: 0.9067\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.2014 - acc: 0.9298 - val_loss: 0.3377 - val_acc: 0.9067\n",
      "Epoch 116/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9298Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.2696 - acc: 0.8785\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1956 - acc: 0.9298 - val_loss: 0.4832 - val_acc: 0.8785\n",
      "Epoch 117/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9308Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 256us/sample - loss: 0.1903 - acc: 0.9102\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1955 - acc: 0.9307 - val_loss: 0.3334 - val_acc: 0.9102\n",
      "Epoch 118/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9299Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2707 - acc: 0.9033\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1973 - acc: 0.9299 - val_loss: 0.3642 - val_acc: 0.9033\n",
      "Epoch 119/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1964 - acc: 0.9318Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 266us/sample - loss: 0.2259 - acc: 0.9088\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1964 - acc: 0.9318 - val_loss: 0.3333 - val_acc: 0.9088\n",
      "Epoch 120/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9297Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.3120 - acc: 0.9016\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1998 - acc: 0.9298 - val_loss: 0.3754 - val_acc: 0.9016\n",
      "Epoch 121/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9321Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.3067 - acc: 0.9000\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1940 - acc: 0.9321 - val_loss: 0.3681 - val_acc: 0.9000\n",
      "Epoch 122/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9322Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.3270 - acc: 0.8901\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1935 - acc: 0.9321 - val_loss: 0.4157 - val_acc: 0.8901\n",
      "Epoch 123/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9312Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.2627 - acc: 0.8993\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1952 - acc: 0.9313 - val_loss: 0.3735 - val_acc: 0.8993\n",
      "Epoch 124/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9312Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.3285 - acc: 0.8985\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1954 - acc: 0.9312 - val_loss: 0.3840 - val_acc: 0.8985\n",
      "Epoch 125/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9313Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2718 - acc: 0.8964\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1951 - acc: 0.9313 - val_loss: 0.3662 - val_acc: 0.8964\n",
      "Epoch 126/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9326Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.1839 - acc: 0.9070\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1906 - acc: 0.9326 - val_loss: 0.3413 - val_acc: 0.9070\n",
      "Epoch 127/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9311Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.3336 - acc: 0.8931\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1961 - acc: 0.9311 - val_loss: 0.4264 - val_acc: 0.8931\n",
      "Epoch 128/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1964 - acc: 0.9318Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.3268 - acc: 0.8932\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1964 - acc: 0.9318 - val_loss: 0.4049 - val_acc: 0.8932\n",
      "Epoch 129/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9330Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.3195 - acc: 0.8934\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1937 - acc: 0.9330 - val_loss: 0.4254 - val_acc: 0.8934\n",
      "Epoch 130/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9320Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 263us/sample - loss: 0.2604 - acc: 0.8996\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1922 - acc: 0.9320 - val_loss: 0.3801 - val_acc: 0.8996\n",
      "Epoch 131/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.9298Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.2909 - acc: 0.8983\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1975 - acc: 0.9298 - val_loss: 0.3875 - val_acc: 0.8983\n",
      "Epoch 132/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9335Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 266us/sample - loss: 0.2707 - acc: 0.9068\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1883 - acc: 0.9335 - val_loss: 0.3409 - val_acc: 0.9068\n",
      "Epoch 133/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9322Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2223 - acc: 0.9068\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1913 - acc: 0.9322 - val_loss: 0.3306 - val_acc: 0.9068\n",
      "Epoch 134/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9339Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2250 - acc: 0.9015\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1898 - acc: 0.9340 - val_loss: 0.3544 - val_acc: 0.9015\n",
      "Epoch 135/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.9338Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.3056 - acc: 0.8959\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1861 - acc: 0.9337 - val_loss: 0.4000 - val_acc: 0.8959\n",
      "Epoch 136/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9328Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.3103 - acc: 0.9011\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1913 - acc: 0.9329 - val_loss: 0.3738 - val_acc: 0.9011\n",
      "Epoch 137/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9319Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.3047 - acc: 0.8895\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1959 - acc: 0.9319 - val_loss: 0.4227 - val_acc: 0.8895\n",
      "Epoch 138/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9323Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 264us/sample - loss: 0.2727 - acc: 0.8952\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1912 - acc: 0.9323 - val_loss: 0.3993 - val_acc: 0.8952\n",
      "Epoch 139/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9333Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2312 - acc: 0.8969\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1906 - acc: 0.9333 - val_loss: 0.3690 - val_acc: 0.8969\n",
      "Epoch 140/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9351Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.2876 - acc: 0.8882\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1855 - acc: 0.9351 - val_loss: 0.4257 - val_acc: 0.8882\n",
      "Epoch 141/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9324Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2441 - acc: 0.8993\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1893 - acc: 0.9324 - val_loss: 0.3808 - val_acc: 0.8993\n",
      "Epoch 142/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9336Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2100 - acc: 0.9021\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1906 - acc: 0.9336 - val_loss: 0.3492 - val_acc: 0.9021\n",
      "Epoch 143/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9338Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.1940 - acc: 0.9047\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1886 - acc: 0.9338 - val_loss: 0.3274 - val_acc: 0.9047\n",
      "Epoch 144/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1878 - acc: 0.9332Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2607 - acc: 0.8952\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1878 - acc: 0.9333 - val_loss: 0.3863 - val_acc: 0.8952\n",
      "Epoch 145/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9356Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2082 - acc: 0.8999\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1850 - acc: 0.9356 - val_loss: 0.3689 - val_acc: 0.8999\n",
      "Epoch 146/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9344Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2350 - acc: 0.9017\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1856 - acc: 0.9344 - val_loss: 0.3589 - val_acc: 0.9017\n",
      "Epoch 147/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9335Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2875 - acc: 0.8979\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1862 - acc: 0.9335 - val_loss: 0.3740 - val_acc: 0.8979\n",
      "Epoch 148/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9346Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 269us/sample - loss: 0.2341 - acc: 0.9005\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1846 - acc: 0.9347 - val_loss: 0.3741 - val_acc: 0.9005\n",
      "Epoch 149/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9341Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2011 - acc: 0.9010\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1855 - acc: 0.9341 - val_loss: 0.3599 - val_acc: 0.9010\n",
      "Epoch 150/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9353Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2123 - acc: 0.9021\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1839 - acc: 0.9353 - val_loss: 0.3753 - val_acc: 0.9021\n",
      "Epoch 151/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9339Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2453 - acc: 0.8990\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1897 - acc: 0.9339 - val_loss: 0.3826 - val_acc: 0.8990\n",
      "Epoch 152/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9344Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2285 - acc: 0.9035\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1868 - acc: 0.9344 - val_loss: 0.3526 - val_acc: 0.9035\n",
      "Epoch 153/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9374Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2032 - acc: 0.9038\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1827 - acc: 0.9374 - val_loss: 0.3508 - val_acc: 0.9038\n",
      "Epoch 154/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9334Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2486 - acc: 0.9031\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1878 - acc: 0.9335 - val_loss: 0.3577 - val_acc: 0.9031\n",
      "Epoch 155/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.9367Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2506 - acc: 0.8987\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1819 - acc: 0.9367 - val_loss: 0.3999 - val_acc: 0.8987\n",
      "Epoch 156/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9363Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2048 - acc: 0.9017\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1785 - acc: 0.9362 - val_loss: 0.3633 - val_acc: 0.9017\n",
      "Epoch 157/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9349Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2539 - acc: 0.9026\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1831 - acc: 0.9349 - val_loss: 0.3843 - val_acc: 0.9026\n",
      "Epoch 158/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9355Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.1990 - acc: 0.9070\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1835 - acc: 0.9354 - val_loss: 0.3432 - val_acc: 0.9070\n",
      "Epoch 159/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9362Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2396 - acc: 0.9025\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1823 - acc: 0.9362 - val_loss: 0.3577 - val_acc: 0.9025\n",
      "Epoch 160/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.9361Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.2502 - acc: 0.9015\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.32717\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1820 - acc: 0.9361 - val_loss: 0.3755 - val_acc: 0.9015\n",
      "Epoch 161/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9362Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 257us/sample - loss: 0.2269 - acc: 0.9097\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.32717 to 0.31908, saving model to model-ep161-val_loss0.319.h5\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1858 - acc: 0.9362 - val_loss: 0.3191 - val_acc: 0.9097\n",
      "Epoch 162/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9370Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 256us/sample - loss: 0.1843 - acc: 0.9124\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1790 - acc: 0.9370 - val_loss: 0.3279 - val_acc: 0.9124\n",
      "Epoch 163/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9356Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.2091 - acc: 0.9076\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1802 - acc: 0.9356 - val_loss: 0.3399 - val_acc: 0.9076\n",
      "Epoch 164/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9372Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.1877 - acc: 0.9065\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1780 - acc: 0.9372 - val_loss: 0.3413 - val_acc: 0.9065\n",
      "Epoch 165/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9352Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2128 - acc: 0.8994\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1811 - acc: 0.9352 - val_loss: 0.4037 - val_acc: 0.8994\n",
      "Epoch 166/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9374Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.2567 - acc: 0.8961\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1789 - acc: 0.9374 - val_loss: 0.3834 - val_acc: 0.8961\n",
      "Epoch 167/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9349Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.2563 - acc: 0.8914\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1837 - acc: 0.9349 - val_loss: 0.4127 - val_acc: 0.8914\n",
      "Epoch 168/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9365Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2279 - acc: 0.8985\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1817 - acc: 0.9365 - val_loss: 0.3872 - val_acc: 0.8985\n",
      "Epoch 169/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9366Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.1999 - acc: 0.9051\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1793 - acc: 0.9366 - val_loss: 0.3542 - val_acc: 0.9051\n",
      "Epoch 170/190\n",
      "779/781 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9376Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2615 - acc: 0.8913\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1795 - acc: 0.9375 - val_loss: 0.4366 - val_acc: 0.8913\n",
      "Epoch 171/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9360Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2436 - acc: 0.9056\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1837 - acc: 0.9359 - val_loss: 0.3537 - val_acc: 0.9056\n",
      "Epoch 172/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9371Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2668 - acc: 0.9073\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1780 - acc: 0.9371 - val_loss: 0.3702 - val_acc: 0.9073\n",
      "Epoch 173/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9366Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.4841 - acc: 0.8971\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1810 - acc: 0.9366 - val_loss: 0.3990 - val_acc: 0.8971\n",
      "Epoch 174/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9374Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.3116 - acc: 0.9092\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1792 - acc: 0.9373 - val_loss: 0.3432 - val_acc: 0.9092\n",
      "Epoch 175/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9386Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.3139 - acc: 0.8897\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1761 - acc: 0.9386 - val_loss: 0.4371 - val_acc: 0.8897\n",
      "Epoch 176/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9376Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 260us/sample - loss: 0.2756 - acc: 0.8942\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1749 - acc: 0.9376 - val_loss: 0.4109 - val_acc: 0.8942\n",
      "Epoch 177/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9364Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2283 - acc: 0.9094\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1815 - acc: 0.9364 - val_loss: 0.3270 - val_acc: 0.9094\n",
      "Epoch 178/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9385Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2903 - acc: 0.8806\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1777 - acc: 0.9384 - val_loss: 0.4632 - val_acc: 0.8806\n",
      "Epoch 179/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9391Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.3388 - acc: 0.9086\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1779 - acc: 0.9391 - val_loss: 0.3398 - val_acc: 0.9086\n",
      "Epoch 180/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9387Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 262us/sample - loss: 0.3017 - acc: 0.9107\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1740 - acc: 0.9387 - val_loss: 0.3293 - val_acc: 0.9107\n",
      "Epoch 181/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9375Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 263us/sample - loss: 0.3326 - acc: 0.9050\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1755 - acc: 0.9375 - val_loss: 0.3585 - val_acc: 0.9050\n",
      "Epoch 182/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9377Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 259us/sample - loss: 0.2104 - acc: 0.9092\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1771 - acc: 0.9377 - val_loss: 0.3390 - val_acc: 0.9092\n",
      "Epoch 183/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9387Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 265us/sample - loss: 0.2752 - acc: 0.9030\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.31908\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1749 - acc: 0.9386 - val_loss: 0.3655 - val_acc: 0.9030\n",
      "Epoch 184/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9384Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 263us/sample - loss: 0.2014 - acc: 0.9076\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.31908 to 0.31710, saving model to model-ep184-val_loss0.317.h5\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1748 - acc: 0.9384 - val_loss: 0.3171 - val_acc: 0.9076\n",
      "Epoch 185/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9389Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.2156 - acc: 0.9034\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.31710\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1727 - acc: 0.9389 - val_loss: 0.3401 - val_acc: 0.9034\n",
      "Epoch 186/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9401Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 258us/sample - loss: 0.1969 - acc: 0.9074\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.31710\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1699 - acc: 0.9400 - val_loss: 0.3548 - val_acc: 0.9074\n",
      "Epoch 187/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9377Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2303 - acc: 0.9005\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.31710\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1761 - acc: 0.9377 - val_loss: 0.3838 - val_acc: 0.9005\n",
      "Epoch 188/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9390Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2474 - acc: 0.9091\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.31710\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1753 - acc: 0.9390 - val_loss: 0.3350 - val_acc: 0.9091\n",
      "Epoch 189/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9385Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2487 - acc: 0.9064\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.31710\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.1749 - acc: 0.9386 - val_loss: 0.3452 - val_acc: 0.9064\n",
      "Epoch 190/190\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9382Epoch 1/190\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 261us/sample - loss: 0.2492 - acc: 0.9047\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.31710\n",
      "781/781 [==============================] - 53s 68ms/step - loss: 0.1790 - acc: 0.9382 - val_loss: 0.3529 - val_acc: 0.9047\n"
     ]
    }
   ],
   "source": [
    "steps = int(X_train.shape[0] / batch_size)\n",
    "history = model.fit_generator(it_train,steps_per_epoch=steps,epochs=epochs,verbose=1,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=callbacks_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UE3lF6EH1r_L",
    "outputId": "f5fdca0f-96c2-42c4-b2be-3b0c5d4bcad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYQYiLFuVuWW"
   },
   "source": [
    "## 7.0 Plots on training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QwkMMEf2AsPh"
   },
   "outputs": [],
   "source": [
    "# function to plot epoch vs loss\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "def plot(history):\n",
    "    # plot loss\n",
    "    pyplot.subplot(121)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.xlabel('Epoch')\n",
    "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(122)\n",
    "    pyplot.title('\\nClassification Accuracy')\n",
    "    pyplot.xlabel('Epoch')\n",
    "    pyplot.plot(history.history['acc'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_acc'], color='orange', label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "cm3MeIPidtKp",
    "outputId": "e5f47e49-8d28-41d2-8c4e-9f6684a15665"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAElCAYAAADqeCmyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydd7gcVfnHP+/emt4JgRCSQEAIhACh\nCoKANKWLAqKgIvYGWLAgUgQBEfUnKk0ERAhVOkQg9BZIqKEkIUAChJAEUm9u2fP748zZOXv2zO7s\nvXvL3pzP89xnd2bOzJzdO/udd97znvcVpRSBQCAQ6L1kursDgUAgEOhcgtAHAoFALycIfSAQCPRy\ngtAHAoFALycIfSAQCPRygtAHAoFALycIfSAQCPRygtAHAoFALycIfSAQCPRygtAHAoFALycIfSAQ\nCPRygtAHAoFALycIfSAQCPRygtAHAoFALycIfSAQCPRygtAHAoFALycIfSAQCPRygtAHAoFALycI\nfSAQCPRygtAHAoFALycIfSAQCPRygtAHAlWCiJwuItd04vFfFpE9o/ciIv8UkWUi8rSI7C4ir3XC\nOceIyEoRqan0sQMx64TQi8gxIjIjuqDeE5G7RWS3buzPlSLSHPXH/D2fct9O/bGXi4jMF5F9ursf\nvYXuvFaVUhOVUtOjxd2AzwCjlVI7KqUeUUpt3tFzuNeLUuptpVR/pVRbR4+dcD4RkXki8kpnHL9a\n6PVCLyInARcBvwNGAmOAi4FDEtrXdlHXzosucPO3TSUOGl3Yvf7/2hsp91rtZDYG5iulVnXDuSvJ\np4D1gPEiskNXnrgLtaQ0Sqle+wcMAlYCRxZpczpwI3ANsBw4AWhA/+Dejf4uAhqi9sOBO4CPgKXA\nI0Am2vYzYCGwAngN2DvhnFcCZyVsGwso4DjgbeBD4JfRtv2BZqAl+lzPR+unA2cDjwFrgE2BDYDb\noj7OAb7h+czXR319Dtgm2vYT4CanT38G/pTQ3/nAPgnbvhGde2nUlw2i9QL8Efgg+s5fBLaKth0I\nvBL1ayFwSndfRz3sWr3GWr4BeB/4GHgYmGht836PJa7f+cA+wNeBJqAt6tNvgT2BBdbxNwJuBhYD\nS4D/i9ZvAjwQrfsQ+DcwONp2NZCNrtGVwE+t6702alPqup0KXBV9rpeBKSW+1yuiPtxs+mhtGwr8\nE/0bXwbcam07BJgVXZ9zgf1917v9P7E+y9fRv92HU/yf+gB/AN6Ktj8arbsT+L7T3xeAw9p1fXX3\nBd7JP579gVZzERX58bQAh6KfcPoAZwBPoi2BEcDjwJlR+3OAvwN10d/uaOHaHHiHWMzGApsknPNK\nSgv9pVFftgHWAlv4fuzRuunRhTURqI369TDaGmwEJqN/kHs5n/nzUdtTgDej96OAVcQ/zlq0IG+f\n0N+8C99avxf6h74d+sb5F+vC3w94FhgcfXdbAKOibe8Bu0fvhwDbdfd11MOuVVvovwYMIDZMZlnb\nvN9j0vXr/i+B44FHrePtSST0QA3wPPpm3S+6xnaLtm2Kdvk0oH87DwMXJV0vFAp9qeu2CX0Tq4k+\ny5NFvq++aKE+EDgiuh7rre13oo2dIdF3sUe0fke06H4GrQkbAp9I6H/uf2J9lqui76VPiv/TX9G/\n3w2jz7Rr1O4LwFNWu23QN8/6pM9b9Prq7gu8k388XwLeL9HmdCIBstbNBQ60lvdDP8aCvgn8F9jU\n2WdTtCDuA9SVOOeV0QX7kfX3L+diGW21fxo4yr2wrO3TgTOs5Y3Q1tgAa905wJXWMZ60tmXIF4a7\niSwp4HPAK0U+S96Fb62/HO2eMsv90TeXseibwOvAzkTWpNXubeCbwMDuvn566LV6TcK2wdF1M6jY\n95h0/br/S4oL/S5oAU68KVn7HQrMTLperOu9NuV1+z9r25bAmiLnPtb0E33j+JjIIkYbNFlgiGe/\nfwB/THO94xf68UX6lPs/Rb+7NURP0067RvRTxoRo+QLg4vZeX73dl7sEGJ7CV/aOs7wB+lHK8Fa0\nDuB89CPlfdEgz88BlFJzgB+h//EfiMh1IrIByVyglBps/R3nbH/fer8aLZRpP8MGwFKl1ArnM2zo\na6+UygILrM/4L/SPhOj16hLn9pH3HSqlVqL/HxsqpR4A/g9tzXwgIpeIyMCo6RFoC+wtEXlIRHZp\nx7mrkbTXKgAiUiMi54rIXBFZjhYg0K4ZSP4evddvmWwEvKWUavX0a2R07S+M+nWN1adSpLlu3d9F\nY5Hv7DhgqlKqVSnVBNwUrTOfYalSaplnv43Qxl57yf22SvyfhqMFveBcUX+vB46NxtyOpn2/Q6D3\nD8Y+gXZ7HFqinXKW30UPRhnGROtQSq1QSp2slBoPHAycJCJ7R9uuVUrtFu2rgN93/COU7Ktv/bvA\nUBEZYK0bg/bVGjYyb6ILaXS0H8CtwCQR2Qpt0f+7Hf3M+w5FpB8wzPRBKfVnpdT2aKtsM/TYAEqp\nZ5RSh6DdZreifbLrAmmvVcMxaD/yPmjrcGy0XiD5eyx2/ZbBO8CYBIH9Hfpa3FopNRBtKIi1Pen6\nhXTXbSpEZDT6yfFYEXlfRN5HuyoPFJHh0WcYKiKDPbu/gx5r8LEK7RIyrO9pY3/GYv+nD9FP9knn\n+hf6SW9vYLVS6omEdiXp1UKvlPoYOA34q4gcKiJ9RaRORA4QkfOK7Pof4FciMiK6KE5DWyaIyOdE\nZFMREfSjYBuQFZHNRWQvEWlA//PWoB8NK80iYGyxyBql1DvocYVzRKRRRCahB4jssMztReTw6Mf6\nI7TIPBnt34QerL0WeFop9XaJPtVF5zF/tejv8KsiMjn6Tn6H9jnOF5EdRGQnEalD/3Ca0N9hvYh8\nSUQGKaVa0P7VzvgOexztuFYHoP9nS9DC8zuzodj3mHT9ltndp9GuvnNFpF/0P/+k1a+VwMcisiHR\nDdxiETA+4TtIc92m5cto9+DmaF//ZLRBsQA4Win1HtpFebGIDIm+609F+16Ovnb3FpGMiGwoIp+I\nts0CjoraT0HfPIqR+H+KnqSvAC4UkQ0i63+X6PdCJOxZ9GBtu615c7Je/4e+K85Ai8r76EGYXV0f\nm+Mf+zP6Yn4vet8Ybfsx+vFrFfqi+XW0fhL6B7ACHTFwB9HArKc/V6KjZ1Zafx8qx2dptZ8OnBC9\nH4YemV8GPOdut/YZHfVhKfrR8FvWttPJj7qZiTPoiY6jVsBXS3y386N29t9Z0bZvRec238foaP3e\n6AiClcSRGf2BeuCe6LMtB54hGuRbV/7SXqvR9/Xf6P/3FvCV6LvftNj3mHT9Wv/Lkj76aHkM+knB\nRNf8OVo/ET3QvhItiic7+x2CHj/4CB0EkHe9p7hu7cHovH2d7/FVnKiVaP1PgRnR+6Foq3lR9F3d\nbLU7LLpGV6BdXftF68cDT0Wf7060Nrg+evu3m/h/irb3QQ/QLiSOyulj7f8rSvj90/yZ0fbAOoSI\nnI6+0I4t0mYM+seyvlJqeVf1LRAIxIjIV4ATlXYJt5te7boJtI/ILXQScF0Q+UCgexCRvsB3gEs6\neqwg9IE8okHT5egY4t90c3cCgXUSEdkPHRq6CD1W1rHjBddNIBAI9G6CRR8IBAK9nJ6TdCdi+PDh\nauzYsd3djUAv5tlnn/1QKTWiq88bru1AZ1Lsuu5xQj927FhmzJjR3d0I9GJE5K3SrSpPuLYDnUmx\n6zq4bgKBQKCXE4Q+EAgEejlB6AOBQKCXE4Q+EAgEejlB6AOBQKCXE4Q+sE4jIvuLyGsiMseXm11E\nNhaR+0XkBRGZHqW/tbcPFJEFIvJ/XdfrQKA8gtAH1llEpAZd/OQAdF78o0VkS6fZBcBVSqlJ6OpM\n5zjbz0RnHAwEeizrttC//wAsf6O7exHoPnYE5iil5imlmoHr0Gl0bbZEF7sGeNDeLiLbAyOB+7qg\nr4F1kJUr4YoroKOZatZtoX9gb7hjs+7uRaD72JD8EowLyC9bB7oI9uHR+8OAASIyLMrw+Qd0TvVE\nROREEZkhIjMWL15coW4H1hW+8x34+tfhqac6dpx1W+gDgdKcAuwhIjOBPdAFItrQ6WPvUkotKLaz\nUuoSpdQUpdSUESO6POtCoIrIZmG5kxT8wQf166JFHTt2KqFPMWB1vIgsFpFZ0d8J1rbjROSN6M8t\ngB0IdCcLsWrnoqsb5dUnVUq9q5Q6XCm1LfDLaN1HwC7A90RkPtqP/xURObdLeh3olVx4IQwaBHPm\n6OVsFhZEZsQ3vwk33tj+Y5cU+pQDVgDXK6UmR3+XRfsORec03wntD/2NiAxpf3cDgYryDDBBRMaJ\nSD1wFHCb3UBEhlv1eU9F1/hEKfUlpdQYpdRYtNV/lVKqwAgKBIrx5pvw059CSwv89rd63U9/Cs88\nAxMnxu0WLYLjjoM32jmkmMaiTzNglcR+wDSl1FKl1DJgGrB/+7oaCFQWpVQr8D3gXmA2MFUp9bKI\nnCEiB0fN9gReE5HX0QOvZ3dLZwO9kssvh/PPh0sv1QOv9fUwbRqcdBK8+ipsZD1vrl4Nxx8PbW3l\nnyeN0KcZsAI4Ioo1vlFETPdS7RsGrALdhVLqLqXUZkqpTZRSZ0frTlNK3Ra9v1EpNSFqc4JSaq3n\nGFcqpb7X1X0PVD9PPKFf//IX/XrKKVrwH30UPv1pmD49bnv++TBvnn4KKJdKDcbeDoyNYo2noSur\npyYMWAUCgWrk/ffh7ruTtyul3TJ/+Qu88oq2yidOhHvugdbWOJrm1Vf161e/Gu97zjkwfjx8/vN6\n+eST4bXXYNNNy+9nmnz0aQaslliLlwHnWfvu6ew7vdxOBgKBQE9kr71g9mxoboa6uvxtjz0G//kP\n/PWvennIELjqKi34BxwAgwfDqlUwYYL2vW+4oRbxK6/ULpuddtL7XXedvimIwMCB7etnGos+zYDV\nKGvxYLS/E7Tvc18RGRINwu4brQsEAoGqZ3akdLvvnm/ZL10Ku+0WizzAsmVw5pnx8kcf6dfzz9ev\nEybo1+OO0zcQQ00NNDR0rJ8lLXqlVKuImAGrGuAKM2AFzIh8mT+IBq9agaXA8dG+S0XkTPTNAuAM\npdTSjnU5EAgEehZPPQUHHqhdM5dfDg8nJMV4+un85dNPh898Rgv5Jz7Ref1LVUpQKXUXcJez7jTr\n/ano0DPfvlcQhaQFAoFAb+aMM+DcIrMpxo6FXXeFa6+Fv/0NvvUtvX7aNO2P7yzCzNhAIBAowaxZ\nMGkSLIlGI9es0dExLueeq331M2fqHDUuO+8Mv/kN7L03fPGL8frdd9c++s4iCH0gEAh4uPVWHVUD\ncPbZ8OKL8N//6uVtt4WhQ/37HXMMTJ4MRx0F33OCbocNg802g//9Tw/OdhVB6AOBQMBh2TI47DA4\nPEpnZ6K+X35Zh0a+9poOm7QZN06/nho5sfv00WGV//gHbLONXjdpUuf33UcQ+nWVNYvgv+Ng+Wvd\n3ZNAoMdhomnmztWvJqnYhRfq0Egf06bBXXfB5pvnrz/xRHjuOf2EcMIJ/n07myD06yoLboZV8+HV\ni7q7J4FAt7NkCfzpTzpqBmKhHzhQJxdLMxt1k02SbwKZDBxyiH7tDtZdoe9oJv9AINAr+OAD+MMf\n4Ec/gkMP1euM0M+Zo6NhZs6EHXeM9zn5ZBg9Wu9TDaQKr6xqlAKVhUyNu6FbuhMIBCrP4sWxH71c\nRo6M30+bBgsX6tmrhrfe0q/bbaddOEceqSc5nXeenq162WXQv3/7+94V9H6L/vlT4bpayLbmr1fZ\n7ulPTyE80QR6CffdB+utp1/Tsny5Ful/WVm5RkXz+6dO1UnFDJ/+tA6F/OY3Yf78eCZrJqOPsXix\nXt+T6f1C/8rv9Wu2OX99NQh96ypo+rBzzyHSuccPBDoZMws1Tbm9WbN0SgEj8GbCEuiUBZttBr/6\nFaxYEUfKTJ6s881Mnuw/ZmNjx1MUdDa9X+gN2RZ3Rbd0oyzumgw3d3I2z2DZB6qc5siGc5OKGS68\nEPbfXw+oXnaZHlz98Y/1tqamuN2AAbqdGZD9+9+1S8a+GVQr1Sv0b/4b5lyavr0r9NUgcCujmmLv\nTfPcqDpIsOQDvQQTz+4K/QsvwEEH6YHTe+/VM1VNuQu3eMfkyXpW63776eVvf1vPYl2xQlv51U71\nDsY+cax+3fQb6dqrKrToDQ/uCxN/AduE4kaBgIux6N0JTF/7Gjz7bLx87rk63a9h//315CfQoZUj\nRujwyNtu09t6E9Vr0ZdLtQ/GrmhnscgkquGJpgtIUfh+YxG5P6qeNl1ERlvrnxORWSLysoj0ggf8\n6sTknFm+PH+9LepHHhnndDdccEH8ftgw/SqinwKS3EDVyroj9K5FX21CTye5WtZhF07KwvcXoAt/\nTwLOAM6J1r8H7KKUmgzsBPxcRDbomp4Hmpvh17/Wed9NojFb6A86CJ5/Pl4+7TR9Q1i9Wrd77TXY\n0vpPDx/eNf3uLqrXdVOMluXw8KFQNyheV42DsXmsu4KcR+tqkBqoqUiYQ67wPYCImML3VhQ1WwIn\nRe8fBG4FUErZYVwNrEtGUw/g+uvhrLO0D/3DKDDNCP2bb8Idd+S333hj6NdPv29s1AOvNkkJynoL\nvfPifPsGWPQgLLg1XleNg7E2Ev2rmj/u3n50N1P7wR0Vq9CQpnj980CU2orDgAEiMgxARDYSkRei\nY/xeKfVupToWKM6cKE5h7drYor/6ai3+W21V2N4Vdpfe5qpx6Z0WfevqwnUFQt9Oi775I1Bt0DCs\nffu3G4FFD8H9e8Ked8MGFRotqrYbHugcPV3HKcD/icjxwMPoOshtAEqpd4BJkcvmVhG5USm1yN5Z\nRE4ETgQYM2ZMV/a7VzNjhn59//3YogftzgE4+GA9oDpvXpyQzMedd+r0Br2d3in0vlBE1eo2Ku+Y\naxbBLevHy8d0sUCKwIeP6/eLHqyc0K/bpCl8/y6RRS8i/YEjlFIfuW1E5CVgd+BGZ9slwCUAU6ZM\nqcK7as/jgw/gkUf0+1tvLdw+cGCcN74UBx6o/3o7vdN1UyDqdNyi//jF4sfrdDLEfvoK6sU6PBhL\nusL3w0WM34xTicpiishoEekTvR8C7AaEnM+dRNb6uZ57rp7otMceenn77fPdNbW903ztEL1T6H0i\nvPjRfP92uUJf75SD6eo87uu2IHcKSqlWwBS+nw1MNYXvo2L3AHsCr4nI68BIwExm2AJ4SkSeBx4C\nLlBKOdZAIA1KxX72JUt0PVWl4O239bo77tBpC+bM0dufe05nkvzVr/SkpltuyXfffPKTXf8Zejqp\nhL5UrLHV7ggRUSIyJVoeKyJroljjWSLy90p1vCg+i37Wz+Chg+xG5R0z60ylW/a8v12nIZbYBw9A\npVBK3aWU2kwptYlS6uxo3WlKqdui9zcqpSZEbU5QSq2N1k9TSk1SSm0TvV7SnZ+jmrnsMh3e+Oqr\nOnHYl74EX/mKjpR54AH42c90u2231e0eekinDt5nH3jiCdhoI13DFXQ0ztVXd99n6amUfMixYo0/\ng45KeEZEblNKveK0GwD8EHBTC82NYo27jiS3ytIZ8ftyLXrlCP3yV/ztOgsRcq6bahxADQQSuPNO\n/frii1rsAa65Rr/efnucMtguxj1+fP4x7r1X++u/8IXO7Wu1ksaiz8UaR7HDJtbY5Uzg90CTZ1tl\naf6o+HafRQ9xiCJQ9mCsK/RNH5S3f4eptOsm3CwCPQOT+XHVKnjXClAdMULXXPXhCv1OO8E55/jb\nBtIJfclYYxHZDthIKXWnZ/9xIjJTRB4Skd19JxCRE0VkhojMWGyyDiWx9Dm4sUT59CSLXqziI2WH\nV0btRx8Kg7aCtZ2cPrgAIRbnINKB3oOJYX/gAV2Ue6+9dMWn448vTD5m2NCd7RAoSocHY6OIhAuB\nkz2b3wPGKKW2Rc8uvFZEBrqNlFKXKKWmKKWmjChVJsZ2vyTh5rXJYX/cMsXSWPSb/xAaR+jJWFNL\nzMKoJJKBtuhhqaOum/n/gdkXlG4XCHQBq1bpV+Nbv+wyOOkkXVTb5eab9RNAUm74gJ80gUilYo0H\nAFsB00UPFq4P3CYiByulZgBm8OpZEZkLbAakUOsE0oQ1FmSqjMh0wKI3Qi810BAlxmhdqY+z4g1Y\nMQc2/Gx5xywLiYU+u7Zjh3rrP1096SgQSMR+iN90Uxg3Ln7/179C377wiU/Ak0/CYYfl55APpCON\n0OdijdECfxRwjNmolPoYyKUEEpHpwClKqRkiMgJYqpRqE5HxwARgXod67KsUJc6DSRqLvlyhz3qE\n3vTHTMnvzElUYgl925qOHat1Vcf7Ewgk8OyzOp/MxIl6+ZFHdDTN44/D4MGF7W2hP/jg/G3f+U78\nfuedK9/XdYWSrpuUscZJfAp4QURmoWcMfksptbRDPS6Y+GQ58eZdBU8cl2zRd8RHn2fRW+6ltg5a\n16nJBKEPVAVTpuRPYDrtNJg9G+66Sy8vWgQvvxx7II3Q19XptoHKk8pHXyrW2Gm7Z+SyQSl1k1Jq\nolJqslJqO6XU7R3usSv0tvX+5HHw5lXJFr0t9O2NupGa/MlT9hOGG2tfSURigTevLcth+evlHytP\n6MNErEBlaGnR5fdchkQ/lyuv1FE1O+ygbwQXXaT3WbYMTj9d++oHDSrcP9Bxqm+ycIHrJkW6A0Oe\nRd/OwVipyT++7S9vWwOZ/uUdd+mz0HdjaCyVENty3bSu0U8kN0S/inJdRsGiD3QC//gHfP/78XJr\nq05HMHeuXp42TUfLmCibW26BTGRqbrJJ788g2Z1UodC7rhuP0Ld5sldCZeLopUZH3eTOZQl96yqo\nK1Po75kC/TaGQ+YXbycZyFqum0UPlHcem7YqEPoPHoa1S2Cjw+Kb2rYhUqgn41Z4uuce2GADXbv1\nBz+AhQvhppu0H36zzXTc+yOP6AHWo4/unj6vK1RfrhtX2H3ukqQJVZXy0Y/7Coz5YnR+26JPuMGU\nYtVbKc6vtCUPWug7kpc+z6LvoTH5/9sDHonSwGebdYTTM9/u3j4FvFx+ufYsrnLsh4MO0gnHQLtq\n/vhHnYfmzDN1ab8BA+CQQ/Qs2JqawuMGKkd1WfTPfAfe+Fv+OtUKb02FOis8v6UzhD4bH0MysPEX\n4e3r811JnekSUW35Fn3S7N+Sx1H+fP09mdyAew+9Ka3j/OhH+tUuxG3z+9/r/DV9+8Kjj8br3SeA\nQOdRXULvijxowXvlXGgcGa9LtOgrMGHKHCMTzdvOc91UUEDdG5FqzY+6SQwhLUHbGvI/exUMxrrp\nJwI9itXRZW8L/b77wk9+ouPfR4/unn4FYqpH6JOs5Wyr9tu3NWkRVlloXuZvWynXDcQ1S4u5blpW\n6PPUJ4QSFOuDK+SqLT/qxrbolUqfxrgaB2KD0Pc4li/XyciOOirOFW+nCr7nnpBZuydRPT76pQn1\nvlSrjpvProVMo17XutLfVjowYcoVeq9F74jo7RPgRs8MEZWFF06HNe8XOZ9P6O2om9b8bWmpRqFv\n79NLoNP46U/hmGPyXTEAX/1qeXZHoGuoHqH/KCH/u2qLLPq1UNun+DGS4ujTiH6iRW/56F2Lvikq\nVukK1aLp8NJv4amvFTlfEaF3XTedIfTZVpj509JZOluW6zKLnUmw6LuV11+HvzleU2O9P/hg/vrg\npumZVI/QT/gOTPxV4Xrjusk2xRZ9IgkWvf2+eZnfx19g0ddH5y9i0RumfzZ/m3myaCoikF7XjZXr\nxg4zLWdgNq3QL3oQZp8PN4+Em4okmrtzYn4t3c6gvQPPgYpw2WU6FcE7Vg5bM7Fp6tT8tnvv3XX9\nCqSneoReBAZvVbhetcYWfU0JoV/2HFzrqdJkC/2NQ/1pkFO5bhIGY9+/Dz58Ml42+xYTXa9Fb6U+\nyFqZncpxbRTE0CcMSpsbGfhTMn/wKHz0MqxekP7c7SVY9N2KSVHwgDV1w1j0L7+c3zaU8euZVI/Q\nQ34IpcH46NuaSgt9bh/luGs64rpJGIxtc1LsGTFea6X6SRpLsNvby22rIRNNH2y1RL8jFn2S26q2\nb/Hj/G93uMtz4+0MOslHX6pEpohsLCL3i8gLIjJdREZH6yeLyBMi8nK07Yud0sFu5JVX4Mc/1vng\njdDff3+8/X1reGngQD0R6qqrQmHunkr1C33OdeNY9MVEP9uc7LpJwgh9xrXoLcG1RXRtVO14029G\n52zS4aE3DYPls/W6liJC74q3SYlcP6zwvK4Qrl4AK+f7j5tW6HsSnWDRWyUyDwC2BI4WkS2dZhcA\nVymlJgFnAKaG0WrgK0qpicD+wEUi4hl1r15OPlnnonn44Vjop07VSceamnRisgkT9Prly+Hww+HL\nX+6+/gaKU133X69FHw3GqiyI9XEyjYVWtaF1FeUPxloTpiB2bbSssI5rWfRG6PtsoF/bmuCNKOPT\nmqheWjGL3hV6EzLaMBSa3s8XelcIb43KB/hy4BS4lxI+e09yl3ROX3IlMgFExJTItIsBb4kumAPw\nIHArgFIql0lOKfWuiHwAjABK1Ljs+dx0Ezz3XJyI7N57tdDX1cHatXpW6xZbaIv+Bz/QdV6POKJ7\n+xwoTS8Q+siil9Z8QahphKQaJW2r2++6MQ9BxnXTYqUisK3l5kjo+0Y1z+ZeBh+9EJ3fuHuKTNpy\nrXQj9PVDo2N0gutm6XNQ2w8Gbl6+u8RXF6BSdM5grK9E5k5Om+eBw4E/AYcBA0RkmFJqiWkgIjsC\n9cBc30lE5ETgRIAxY8ZUrPOdxRe/qN0160fj69OmaaHff39dqBvg4ou16I8aBeed1319DaSnF7hu\nWmKxt0Mdi7luWleRPxibwmJMGoxtseZx2++NL75PJPTv/8/fLvF8pYS+KbltMdzBWFvo79k+LqJS\nrri6WUUrhcp259PFKcAeIjIT2ANdeCfXGREZBVwNfFUp/2NhWWUyu5Gbb9bW+xZb6GXjg581C1au\n1MW3r7oK9thDx84PHw6f+lT39TdQHtVl0dd6arTagme7JWqKxNS3OhZ9e+LozaCoLdpv/QdG7Qf9\nxxVa9DYtnoRk7iyTJIu+IYWPvhhpffQFUT8lZsGkiXpKwxt/gwGbxcvZ1s4ajC1VIhOl1Ltoix4R\n6Q8coZT6KFoeCNwJ/FIp9Yt8VoAAACAASURBVCRVTpL7xcx6HTFC++D32QdmztSv9fX+fQI9j+qy\n6DOe+5It9La1Wsqid4V+7VJ4YL/kfVyhF9FWvRH6bc7W4vjEsTDtk3EsfuOowmP5LHq3DmxBlazo\nCcQn9B1y3SRYy25WULt/vlz+SeMh5fLMd+CBfaxztXaWRZ8rkSki9egSmXmFdERkuEjOH3UqcEW0\nvh64BT1Qe2NndK4rSarBusMO8XvzMDJqFBx4YBD5aqO6hB5gvT3yl5OiXooJ/fv/g+kHxMsqC3Mv\n1/HuSbhJzUAPyBrrvMF5LDfCZ+e52eBz+tUn9EnhmC5eH31HZsamtOjt+QLeYi+dVFKxk4Q+ZYnM\nPYHXROR1YCRwdrT+C+gymceLyKzob3LFO9mJ3HIL7LabttjnOqMLm26qX+1Zrtts03V9C1Se6nLd\nAOwzHf5TE1vkaYR+v2dg5inwwUN6+aXfOgdNMZCo2nQb231RY1n0dU7ismyztv4zdfov2xJb4z7X\nzewLYJuzrPMlCH13uW7y5gt4RL2zaudmW/3fRQUSqiil7gLuctadZr2/EV3r2N3vGuCaDp28mzk8\nSvW/bBnMmZO/7Uc/giefhLPPhhUr4NBDYfz4ru9joHJUn0XvkmTZ2ukQhk2B4bsmH0NlnTw4vjZt\nhW1s1407UJxtjkMwzX45ofdY9C+fnT8JasF//f0wFn05E6bammD+tVoc0wq9e/OwhdxnvVfCom/z\nDOgmWfTVEP9fBSxaBG+8od+boKDx4+Hqq/XytGnw3e92X/8ClSGV0JeaQWi1O0JElIhMsdadGu33\nmogUcYKXg2XJ2UJv47pufP59Q5rQQJ/Q51n0A/L7lW2OI3OMKBWz6AFao5j8Ve/oPDMuUhPfUMqx\n6F/4DTz+JXjvnjJcN0V89L4Im44KfbYF3rnZsz5hMLYnxflXEXfcAf/+d7y8aJFOYzB8OGy8sV7X\nWIEx9UDPoqTQp5xBiIgMAH4IPGWt2xI9yGVmEF4cHa9yJA0CusIuRSoPqzZKfhUqW9gm0xCLdk1j\nHFsPURRKvbUvUB/NQkkKrzQTqJJErG5gHO1TzmCsmaDV9EFyeKWv0AnABgdG5yvluungYOyLZ8Dj\nnsKhqi3Bog9CXw4rVuhyfgcdBMceG6+/5x6dw2annXTysiOOgJ137r5+BjqHNBZ9bgahUqoZMDMI\nXc4Efg/Yv/hDgOuUUmuVUm8Cc6LjVY4ki94tIl7Mok/to3ddN/WxwNX0jS14iPLSmNCESERr++s2\nSX02s2yTYtJrB8Q3rDSDsUa8zc1BtSa7btxzGit6/SgCxljsSunkcC5pfPRtTclpH5Lq5iYOxgbX\nTTm8/nphAjLQE57efht22UUX7L7xRuhTItt3oPpII/S+GYR5weEish2wkVLqznL3jfY/UURmiMiM\nxSaxRlEsF0lrgmi6wpMpZtGnmJTjdd1Yv4iGYfnuopYVsdAbMa3pWzy+v5TQ1w0oz6I3Nzuzz5Jn\nYPmr+W3M53a/L3PM2n752+deDo94gq7TuG7ungw3eOZC2Odx+egFeOnMwvXBoi+LJdG0jt1282/f\nZZeu60ug6+nwYGwUZ3whcHJ7j1H27EEp4qM3QloglkU+qsqWFirVFic0MxifO2i3jG3Rt1pCb6jt\nW7w4ivHRJ/Wltr8l9CnSFBuhN08Bc/6h96vtH7fJWfROnLwR0pp++duTCsCkseiXv5a8LUnoHz4U\nljxVuD4Mxpbka1+D00/X/04j9JtsUtju2mvh05/u0q4Fupg04ZWlZhAOALYCposW4PWB26JY5JKz\nD9tHEaGvGxhVYHIt1CIWoMqW9jH7LPqG4frVCLDto29ZkS/8oMXMZ9FPOhNe+HXso/dFn4C+URih\nt29kiRZ91Mbtd91AK6Gax3WjWpMtekm4ZDo6GJsk9EkEi74oSsE//6nfNzfDOVHeTRMjb3O0Z2gk\n0LtIY9EXnUGolPpYKTVcKTVWKTUWeBI4WCk1I2p3lIg0iMg4YALwdEU/gSvQJk1CgSuimDAUEXql\n9KxZn9A3Rk8fJuQxz3WzvNCir+nrn8g1OJqN0uKx6Pe8C8YcGe/vG1ROtOibC48H+aGgufkI9mBr\nU3zMWseiT3KBdTSOvlT+e5cg9EWxq0FdY0X8jxsXvz/rLPjLX7quT4Huo6RFr5RqFREzg7AGuMLM\nIARmKKVuK7LvyyIyFZ36tRX4rlKV+IUWsej7j4OVc2DgZrD0GaszJSz6JIt09vkw62c6h02SRW/E\n23XdNAzNb1+b4KOvj1KZuz76Ufvr8749Nd7fFtqaKBVzYhqDyHXjDsD6iqS7E6Jciz4n9Alz37MV\nSoGQluC68XLrrdDSkj+gaov+8OHx+1/8IhTxXldINTO21AxCZ/2ezvLZxFPHK48r9MN2gi1/rlPt\nzrcChosJ/TPfhsGT8teZ2PrZF+jltUs8Qj8ibgv521s8PvqkwVgj9DnXTSSqk86MZuPWxPvbQm9y\n7tuuGzsPjblhuEJvV7nyuW6ytkXfN79PlbDos62FUVBulFQpgkXv5bDD9OtZZ/m3D7bKowSRX3eo\nzpmx9hXqFtLI1ML6exX6fIvFmi9+FD52Ys+MkKyNooDampItep/otPp89AlCX9NX3xTWLtFCbUTX\n+PxzQt8n30duniRs1439OZOEvnmJ1T7BdZM0GFsJH73PTVa20AeLvhh33x2/H2o9WA7xlEMO9H6q\nU+htXIs+l13SEeU04ZM22dZ8MWlbTcHXVUzoVTa9RZ+p1f197Y86FYLrJjGfxee6AUfc7fctVt8j\nBk+Cbc6Jl9O6bp46AT6enXzDbFurUyk3eQqJF7T1hMQGi75dtLTo+Pj33oNf/jJe/9hjcMwx+v3x\nx8frB/eqgoeBtFRfUjMXd5apsThdy9MIYN0gfwoC18pUbfGMUtBPDvVO4rICoXfS9xYIfZ9YnKU2\nFk2pjcXvhV/DoIn5+ye5bozFn+e6sQTTZ9EfMAsWWVWefXH0PtcNwMLbk1002bVw8/r6nL4ShjbN\nH8UD2b5+pyJY9M8/D3/4g85L4+OEE+DII3Xo5IUXwoABQejXVarUordcN7YYQ2mLvo8nPzzEMey5\n9q3Q7JQJLHDdRHH0w90KdBFGqIdGqX8yNbFFb7uW3OMaN5Jx/dii6/roId+6zTqum/n/gWUz9fLY\nY7Xbyz6fb2bsPVOgaZEeH7CjhPqOSXbRtDX5J3qpbGFU0B2bFSZtK9eid/PlNy/TMfdNaSbc9Q4m\nT04W+QMOgD331JknBw3SxUJmz47zyG+9dZd1c93lWtFPwj2A6hd6V3hyFaAci37o9vp1/X39h3Sf\nDJqXwrJZ8XLbar/Q7/sE7Pwv/zFNrpu9/gefjWpOm1BMe9JSkt/b3CjMZ6zpkx9eWcpHv+pteDx6\nfh//Vdg1UoW8z+Fx3YCepCS1zjhDNt+izzi5fXw88nm4zjOA++49+cvmJjEkZVp313Xzxt/0zePV\nC9PtX+X4ar8YFiyAO+/MH8qaPBk2jOakv/QSPPxw5/ZvnccYLnMv795+RFSn68ZcwZn6QivSCLyb\nu2bs0dryXr0QXv9z4TFbHIt+2m6w5r142RdHDzC8SAYoI4T1g2K3T5/1Pe1qYfebClMLGNeMEdFU\nPnrLMp57Sfw+6QnCNxhrjim1zizktfk3hLr+uko0ONktraiaBbfgpcYZqM626Ket/WbAdWkuS8d1\nY252xVJd9CJWuUlI0bVcW1tjQU9i4sTO6VPAomlRd/cgjyq16CMaLdF0/dk++o/3FxgHa6ZohC3y\nhnITb/pizk2fbb+51MJGh8Mw56aRs+hNFE7ffNdLKYt+hVVRwnZDlXLdgHaBGLEeFWWXzq7NvyHY\nTyX2AKupb5sX6um4b9yJY9kW/bTipplIomDw3KR7qE7bpRyUgkceKVw/ebIu3h3oAayuQAKAClKl\nQh9ZmbZ1bESn1A998NYw4TvtPG0FhN702b6xmOPWOQm/XKE3A6PGas14BmNti96+Wa22skOmcd2s\n/SBut+u/437Y7ewIIjvMtTmK02/6wFr3Uf7x3dDTbEt51nhBWmWTwC290JeqsyAiG4vI/SLygohM\nF5HR1rZ7ROQjEbkjfacrw6WX6rqtLv3KzCIR6ETcscO0vHsPLHRzQ3acKhX6CHtg1YhkKTGWDEz+\nvX/bmC8k+9vTHNvFa9GP1K+2OBtxcoXeuE1sHz3EfvqaEoOxqlUPoAL03dg6rmPRr5wHT38z/9wq\nG980M5YLyRZ6W5jtJ5S1S3SYpZ0AzY7dB/3d3LQezLk0Ol+5Qp9k0ac7Rso6CxegC4BPAs4ArLhU\nzge+nL7DlWHxYp0/3qaxEW66qat70sNZ9ba+DrsLY9HXlJnaY/oB8NDnKt6dKhX6SAAbfRa9I8Zj\nv+TZPUGwaxqLi3m5Qu/6oSG/z+5xk9xKuVmp0fEyjtBnE8IrAYZuC/s8DDv8tfB8oAXzrev95zU3\nIPM5XNdN7obTJz9W/+Xfwc0j4EGroFjebFz04PfaxTAjqlNXtkXvmfdg97k0aeosbAkYWX3Q3q6U\nuh9wBnY6l8sug/XWg+udf9fuu8c1YAMR/90Ybh3Tfec3Fn1SypAupkqFPiJP6CNr2P6hH90Gu3ji\nz5LEoKaxuFCUKk5ScB7PP7lheOE6c9zahFzt7gSqnOvG5LtPcN2Y8623e/7TgmvR97Os/QnfttrZ\ncxIktuj7bKiTrdn9sF03H0wv/AzNjtAbt5L5zNnm1NZ4rt95y+Y7SD2vP02thOcBI6GHAQNEZBhl\nUH6thWSeeMK/PhumFPhpW126TWdhhL51RfEQKcPaJbCm8wZwq1PojUtjhFXw2/ivbRGTjD+hR5Jl\nnmlI2CbF9zO4/1Cf0BcbbExML+CkRMgJbF3+xKuPX4V7ts/ft96jS66P3u73lL/GIY65OQmiz20s\n+oGfgA0OsPpRq39UxrVUUJeWwsdo80MwT2IdtuijG1xS0Zb2cQqwh4jMBPZAp9gua0pu2bUWEnjy\nSR0L7z9Huw8b6CzMvBzVlnzDybbBzJ/A6nfhpuFwxyfibTN+kG88LXlGu6PaSXUKvaFxpLYuIXZr\npIm6SLLMaxr8++dms5YQ+o2Pyl9Oemzb+0H4nK8IR8Iv1nXdGMtXotQJxm2xdEbhvt4nCMeitzNP\nisSx/vZ3kWmILXpzwxke3Wj7jtEXZbHqWQUWfeTDrLOFvozH3CSLPn1ytZK1EpRS7yqlDldKbQv8\nMlrnjCp3Pu+9pytAzZyZn0/+a1/Tr8GiL8KLZ3bPndCufNfsmYkP8MFDOmmimVTVYl1ar/8lPwb/\n3h21O6qdVLfQSw187lU48IX0YlyMmr5+103aY3/ix3Ck5bZNEq6Re+o0yi5Jibpyg7GORS+1ur/G\nuvXdpBpKWPR20ZX1PqVfjdDb34Vt0ZsbzjZnw/4zYNgO2mopNru1QOiNRR+5bto7GJubBxB9hvTJ\n1YrWWQAQkeFRBTWAU4Er0new42SzOo3BBhvE6w46CI47Dp5+Os5lEyz6Irx4mr/GcWdjG0++lCsQ\nGyeJCRcrl160SoXeuFIy2iIcvHVhpsf2UNPHv38m5bFFdH9MNJBvMLYoCb/Ynf+lZ/T2Hx/1x3KZ\n2K4b302qlEW/cq5OVgawx+361aRNzrPo67WI2hZ9plbPOK7ppy16eyDYHVj2DcZCPHZgu27SWPaq\nTc8T+E8NvH1D7C5K6bpRSrUCps7CbGCqqbMQVUcD2BN4TUReB0ZipdsWkUeAG4C9RWSBiOxHBXj4\nYT1zFbQFf7kzsXLMGLjySthhB9hqK73u298mUIzutugThd4YaAkyXMHJf1U6u8TjM89Y4tNe3DTA\nufVlPi0M3FIPNpYzuAjJ//ARu8Be98bLrkXfshwWJoRz9/cUCXU/xxtRRI7JnVPrCVU1rhvbojfU\n9o38kNYPatBW8OHj8bJr0ef2NWmQW+LvyzfjuYAsLIlcVbbQl5EXv1SdBaXUjcCNCfvunvpEZWAm\nPCkFCyNH0hNPwA036MRkGesSGTkyWPPp6IYvqW2NNrLWfliYXsVQKtX2kqdh+C7akO0g1WnR5wZY\nre5XwnVT67HoG4aX/7QwaAv9Wu6kia1+DeOPL91OHIt+3j/hoYPg3bvy2w3fFQZ75rsnDkZHx83N\nSbC/34Z4wpT7pFLbV1sn9oVrXFPmGGYwdtMT/X0p16LPtsU3g0x92RZ9T+eNN+LY+NGj4Te/gR/8\nAL761e7tV1XiS4vd2WSb4ifwlfMKt7//gK5eB8mCP+8KuHsb+I/1O0y6aZSgOoXekPFY9B113dhP\nBCM/DQfNiS3dtMceHYVb9ytz8KRhGOz8z9LtjBVsBmMNb14Vv9/rfviMZ548+D9HTWN8A825U5wE\nZkkWvW9SiImmMYPlxqLf/EfaSjHY0TLlCD1ZazZsPbQZoe9g7dpuxB5UPeEEuCr6d663HgwcCH/6\nk041HChB65rk5Y9fhesaYeWb8boPn4J5VxYWMWoP0w+C53+pzzlwC339L3qwsN0De+vBWCiviM7q\n9s24rVKhL2bRV9B103cjnYys3KeF9feBg9+Mi3pXmpGf1q9r3svvrx1y2Dgy2RXkHYewcs8Y1439\nA8k0FProc+0toR+5l37d4EA9WLvX//SyEfqahvw8N7kC5mVa9E99I45W6CUW/UorK4bJLllTE6cW\nDqTgrakw1TE87PDGeZfra/jtG+J10w+EJ78KC27t+PnfvUNPGGxbo/Vk5Kf1vJJiPrZyiuisWVi6\njYcqFfoIW7DaOxj7SWuaoTsYawQnd+wyvq7+Y/0x/JXAPDG0LIfVCbG15U78ssXbWPT2I2+NJfTF\nLPrRh8ChC3Sc/cRfaBdOpiF+fK0fmn9TMcKsrPDKNINQ9gWfqWuXj76nsWxZ4bq2UEirPBbeVrjO\nttRzgmv9No0R4gYMdIS2Jq0nQ7bVOZ/s0MmCTLFl/JNXvVW6jYfqFHqxom4MxkosdzB24y9Yx3Bc\nNznhqYD/v1wGeMIvDUO2gd2mwnZW7vVxx+W3KSaWSa4bg3G72JZQpiGKrMn6B2Nz7eqhrzPB1Lia\n+o7RET2VsOhtpK6qLfpp0+C++/KFflRCfZxAO8ibsBQJva0d5qnYLT5ULnZ4cdsaPeZn5onYN5sV\nr7s7pj/H8lfb1bVUQp8iy9+3RORFEZklIo+a5FAiMlZE1kTrZ4nI39vVy8SOdYKP3g0phMqEbpbD\nF1bruQHFGHMk9BkJ+z0NB70BAybkby/mwvJ9DjtCKMmiN+mHa52JUbZF77vBGKEfPCk+liHno7ei\nbrb5Xdz/oc5MXy8qzgbatgbumgzziiSn62GceSb87nf5Qn/yyfo1U52mWGXJtuhKaWlCjHxt8iz6\nSFR9T7VuTYpysSdGqVZtIJrfRp7Qz8nfz62WVgxTfa5MSpq/Vpa/z6DzgTwjIrcppV6xml2rlPp7\n1P5g4EJg/2jbXKVUyrJBZeKz6CsZdZMT+k6y6He7Ada87+9HWobtoF/duPVyLXr78bHWim3PHa9e\nJyHzncsuauKzxgdMgNXvwAAT6mn933wW/UaHwtEt+jG1fhjcUGIE0i7ysPxVfa4qKh4+erSeAGWE\n/ppr4KijYNw42Hzz7u1bj+Cls+Gl32oDYaNS2ds8Qt/mEXrjusm2xvNQOmrRtziTpmv7xE+7pg/P\n/hjev8/Z0bLo7XkxLnWD4ONX/NtKkMZeKJnlTyllx/z0o9MDV8V5pbwUCEkkWfSN0XP06gXtP7aP\nMZ+Hzb9XmWO54luuRW9fbG66ZNDfrwntcpOv2a4b39yBnS7VcfVmcNr+QeX56J19+20cP/oWY/U7\nhe8HblF6vx7C6NEwfz78Mwq42m03PQh7+OGhGhQQj+8kpRLIo4RFj2PR21FaSRb9cyfDa56qdC7N\nziBLTZ9Ci37R/YVibUfd1PbXZT99DN0OVs3355IqQRqhT5PlDxH5rojMBc4DfmBtGiciM0XkIRHx\nTjJpf4Y/65/aGTNjjdBvcYp+rR/S/mN3NgVCX2a6ZZ9Fb5M3WOucq8ay6Gs8Fn3/8fDZF2HEJ/Wy\nuWHUNPrDK8vF3IDrBsXrBlWX0Le1wR3RnLchPfgy6xbcpH7F8Llu2nyDsZH02ZFlSRb9W9fBe9NK\nn9strlPjseh9Im3/9jI1sHNCpo1R++uiSW74aAoq5gFUSv1VKbUJ8DPgV9Hq94AxUVKok4BrRaQg\n6Xr5Gf7E7Biv6j9eW52N67X/Q7iDsUa0+m2k/eA7VnaIoaKU5brx/Nvtiy3Jok/aXm8JbJrZwGZK\neJ/R+kesslGhkw4Kfd+oAFSfUXEahypg9Oj85RAr72BPjCtJWoveuG7snDQeoc+2QtP7yfMzlr8G\n86+N9neFvjF2a5o++GL17d9eMQNt1L66rkSjJ61JCdIIfcksfw7XAYcCKKXWKqWWRO+fBeYCRcJJ\nUjLxVP1qC8yIT8KRH/uLb6clyXUDMGDT6rHoPzvbL9YGX9hn3uOjpyZdMYvetqTT/BiNRd9nVDTb\n1kx8aqfQG59m3+gyHfiJ5LY9EFfoKxaVq7LwzHfa7dftMRiRLWdikY3Xoo/Iy0njEfqmD/IT/9ms\nXaJTCz/+JfjoRb9FX5PGorc/VxFJtsOSyySN0KfJ8meHfHwWeCNaPyIazEVExgMTAM984DL5xI/h\nGFVYYLrsJGIOSa6basAW30HtELo8q8JzWdgWvevasdMTpxFr4z/vs4Ej9B34vjN10BA9DZqp51XC\nhpYjdG0lpwE0LYY3/gbv3Vu6bU/GxJ2//hdYeFfxtiUt+mi7ud5ti97nujHzNXwW/TKrQMCcS/0+\n+lrLR69UQm56ezC2iEXv6l0ZlBT6lFn+viciL4vILLSLxgR1fwp4IVp/I/AtpVQFZyVUmEyNP46+\nGkiqTpWWUlEqxSx62wRNI/S7/luXN+wzSou8iQ1OulEf/Kb+K0bDeuR+xH3LTD3RzYwapQde77+/\nwrNgTYqIYpPIVBbeu6/nZUdrXaUjVFa9HbtuFj8KD322+H725xj/NRiynT/qxhgXxqJvGOG36E2+\nKp9Fbyz4+iFa9Ass+sZ8iz7b7P+dtVlzP4yRZcYFj7HHIdsv9KlCVFJk+fthwn43AdVVtrg3WPTt\nwb0AP31v7AqB/EpVxdxCab6z+kG6vOHC2/XF/9yPtOhv9Hl/+/5jSx+zYXgcZllujqFuJpPppOLe\nRsx8ImV49Y8w8xT41K3xjOuewNKZ8NpF+m/YjvnbzJiOd3KkJYwDJsDKOX4ffS4IIPpuGkf488is\nTrDoVTa24PuP1yK/0omPdy36xEpTduGfSH+2PV//ucdrJ2E6hkuSj76n4/Orl4Mr9KP2zY9cscXT\nnRlrU46f3aQjXjEHNvgs9N2g9D429lNM/WA9aOb2dV0mN0ehiNCbpyl7LkIl+PjVwieJbJu+saSJ\nGrFjyV2XyH27wLRPlj6GZLRFncqiX89f3zXJor9vV3g6ysTab5z+/t69Bzb5etympk8szm2rk5Om\n2cculmalM1036xzV6ropdwRv5yvzl0sNdPUbk+5c5XxnmXp93qZFcZbLctj5Clj/M/p93cC44HgQ\nek0ai960aW/Ek4+mD+HOLeAZpyLKm1fBcyfBK79P3nfFHC22ttC7kwqXPK3/vFhCLRltUdsDoLnK\nZB6LXrUVflfGR9+2Fh7/Cty6kb5hLXkqPkff0XoyYesKGH2olY6lLrrZNGqRT4p/t2egF/PRd0CP\ngtC72NZqNQk96ARtB76Yru344/KrT5Xy0acVz3ItekM51rz5H9X2i9/XDYxdD26unXUVY9HblvXD\nh8FjX4qXc9XJKij0Jnx20fT89eaJK+kJY8VcuH0CvHh6XAcZkuPbmxbDgtvz19kWudToyY6r34nX\nm+/CvQmagfyFt+VPjFxtWfTzr9bbPrLSk9QNyo/GG7w17PofPfHJhPuap4ok101ai74D4VhB6AHG\nfjmezJNn0Vfw4u8KNv4CDN4qfXu7CMgmJxRva+rIlqIsi976fvuUIfSmDm5tv3gAt24g7PA3OHxx\nWf+3FHmcNhaR+0XkBRGZLiKjrW3Hicgb0d9x7r7djuuHXvqsTsX71rWFbToyo9wlKZ+MyUdUk+Bm\nNNtfvbBIHVWL/30KHj5YP0HEJ7feZ7Tw2lleja89dxOMrGkz/+axo+CeKfEhfFE39veH5At940id\nwuMLK+KxrNq+xS36tHH0HaBKSwlWmF2vyl+uG6Stkmqz6Mtl0lmw9W+j1MMl/H+lrAmTr769Fn05\nrpuGYdp3mmmIj1E3UJ+7jMkkKfM4XQBcpZT6l4jsBZwDfFlEhgK/Aaag1eXZaF9PsuFuwrVaFz+h\nX+0kdB2dw+A/cfTqXDNG6JLGk3I5Z1YWLzRvMOMLSRlLJRMbPh+9pJ9K7dxKEH839kRLe7wi56O3\n3CuzL4jfZ9fmT87z+dGNRZ+msEknCX2w6H2YOOyuTEvcHYjoJ5jafvnVupLY4W+w02X+bWbQqRxf\nb57Ql2HRDzY58iwhaV/UUck8TsCWwAPR+wet7fsB05RSSyNxn0acyK97+fhVLaqu68bnpilWWL69\nGDdJgUUfCX1SGK0dZri2jFQoeaUCHR/9ICP0kUuzwHVjDca6tK7RA8G+Cmpb/CQ+Xl2JWdhmnMC2\n6BO1xSPJ+z+bXzejHay7Qj/xl4VhWwZTUHvth/7t6yoTvpUfVWBjD9amxRacxjSpLyJ2/Bvs/C+d\nudM8Ute2S+jT5HF6HjApEw8DBojIsJT7diCPUztRSg+EPnxoodWacxFYYtgZg7HmPEmumyQL3F5f\nTuKupJmvUqNDeRuGFbpuVIuOm3/1Ir3su/6MNe+bgDc0cu+o1tIz5mv7FYZX1iYk6/PdAIZul183\nox2su0K/zVmw31P+bVP+ov32G/QMA60q2PMumPJ/Okd+WnIWvaSr3rXzP3WUTW0/GP8V/URiRKyj\n8wiSOQXYQ0RmAnugnP827AAAIABJREFU03+kzoFcfh6nDmKEe9EDheGVxnq3I6xyLpIKTpgyES0F\nQh8JnT04/PrFcVk/W+jLKejdugZWvVO43pw/0wgfPqnTQZg+ZFvglfPiG4A9T8SIrfHPe4V+u/i9\ncd0kXcM1Hou+1vOUUOwYHST46H30Wb/Qbx8oTt8NYbPvlrePEfq0ET3jj9d/NkY02if0JfM4KaXe\nJbLoRaQ/cIRS6iMRWQjs6ew7vT2dqChGLKWmcGasz6I3bdqbR8bbB3PzSLLoLaGf83ddeWzMkY5F\nX4bQL3kSnv0h7D+DgsFY0K6iZTPzUxZkW/L96fb1Y9abtMi+/Fn9xunXxpGx0CfNL6kfqmsr2D76\npLbBRx/odRjXTUfi3rMdsujT5HEaLpIzs04FTA7Ze4F9RWSIiAwB9o3WdS85a7om9nmbp56sz6I3\n6ypYqCXnDnIGY83EJ9uit2PXbaEvNslLMlpgDR/P1q8r33RcN0boPQOk2Zb8mab2bG/T3tyQ7KR9\nhkwN7H4z7PtE7KPfICE9Q+MInRytzXZHJQQ3BIs+0OtYu0S/dkTojWi0Y2awUqpVREwepxrgCpPH\nCZihlLoNbbWfIyIKeBj4brTvUhE5E32zADijR+RxssMl3UlBxVw3nSH0rh1pinBnXaGPrPe0Fr3U\nwZDJcbK2pmii3Nol5A/GRtaxz3pWLY4rxfKZm/amP0lGxEaHxe8PfCk5mV7DejqFsZsLx0cIrwz0\nWoZsV7pNEkY02pkHJEUepxvRCfl8+15BbOH3DLKWRZ9NcN0oK3Qx976E62bRdO1qG7Fr6T74fPRt\nzfEMV9uiz9oWvdUvc3Ma/zU9mDrbyvuSqdPx8UbozYzoZlfojY/eI/RtzbEr6cCXnDoUjfn9LCi0\n47nWBhcpBWZCN1e9ndwm1+cg9IHexvjj9Y9q7DHtP4YJfetAwqdeRZ7QF3HdqKwWQrOuVIHq+z+t\nX49JMWibE2zLPbFspjVZybHosx7XjbHot7tQR87YQi+1MPrwOJ7dRMesXeLkqrF89C7Goq8fWijS\nOddN1B87p9L+M6CxzJoXJqJn1fzSbTvJdRN89IHuI1ML447t2MW921SY/HtdGCbgt+izbnil1S6t\nRW946/rSKY2zHot+8WP6taZPoY++1eO6MX32xfdn6mDELnG6jzW268ailI++daU/zDFTD8ueh6WR\nV85Y9FILQ7cvP8WGidFPJfRhMDYQKKTfGNjypxUsy1TlGOFuXgqLH9Hv2xwfvf2+XB/9Y0fBvH/6\ntzV/rHPIm1w3tkW/5GnoNzaanWoNtJay6H2pGcwgfv9x+X1vXkpxH73Vn2xk0fvGdrKtcPdkmHu5\nXjZC397skcaidzOE7v0ADHJdPsGiDwQCpbCF28SnF4RXWu18kTileP9//vXP/0Lnjzc1VG2Lfs0C\nLcyZhvKibozQ735LvM2sc911BYOxjutm4ObW8YtY9G4Gyw4Lva+OtYKRn9ZjDTbBog8E1gEW3glL\nnindLglfjhjVqgXdzgjpum7KibpZOiNh/bP61TeBaM172rdd0+AU8cgWj7oxx9joUNgyyjlnLHrJ\n5KfRaHZ89O5gbN1gOPAFLbCqiEXvFhAxoZfF6jAUo25wsoC764OPPhBYB3joc3BvQmqONCQlA8uu\nTbDoUwi9a+2veMPfLpdPxljEkatEKS30fUYlW/RKFUbdSG2+S85Y8nbqDDsPTYGPPhJRY9HXNGgL\nun6o36Lfa1qh8GfqrCeIdlr0InEsvivkrmsqWPSBQKAkKkHo29Y6PnpX6Iu4btKmIzC5XEx8umS0\ngM/5uz5Gn1GFFr2J9sk2F6ZAcAdibUveUGu5b5qXOjcsKwUC5BcEyTbrftZZQr/+PjDOyTadaYj7\nMXAL2o15ynFzMrmfMUyYCgQCJUmy6NuaEqJuUsyM9aXXzbblZzzNKxQSxacjsPw1nWMGtOsm0wBt\n1rwyc962NY7QNxVauzlRtKz8nJ9e9M2qxZqU5Proc0Jfrz9/ttljwTvumUy9ngi12w1xNbP2YCz6\n+sFOH13XTTda9CmKM3xLRF4UkVki8qiIbGltOzXa7zUR2a+SnQ8EAg6JrpumfDF+9y4tjGksel8m\nyTZnnW31G6GXTP56n0WfE/qm/DTFbWs8bg2PXWr2N7NSbfeN66PP2Ba98dE7g7E1Tg0KMwYw5vM6\nnr+9GKF30yn0FNeNVZzhAHRu7qNtIY+4Vim1tVJqMnAecGG075bo/CET0bm6L46OFwgE0vLCb+DZ\nk9K1TbToHR/9sz+El89J56P3Cb27zo5UsV03druGEX4fvdnfteiTXDc2q97Sr6OiTLN2anHXR29E\nW+qiwdiVHoveEfqk3PnlYm4SuZuFydnfNa6bNEctWZxBKbXcWuxHHON0CHCdUmqtUupNYE50vEAg\nkJaXzoDX/piubaKPvqmwPN/C24l/qsV89B7XTYHQr/Fss4VeYOBm2n3itehd100xi94zZ8KkFM+5\njUieMJWpg5aV+tyuRV9QVa5Cwmt8826Rkh7kuklbYOG7IjIXbdH/oMx9u7Y4QyDQW0nro4c4SgYq\nbNEb143E7w+YqQU3U8R1k222Eoq1eCz62vi4LgMmeDqekOsmUxffvNwSlgVCX6E8/cYlVO8IfcHM\n3x4eXqmU+qtSahPgZ8Cvyty3a4szBAK9lWLhlVnHos+rzGQJfVszPP0tWB3lkPEKvWPl+yx61VZY\nJ7amwZqpq8gJaVuT7rtdkCONj/7Al3T+GbtwSK69MxhrbhBDJutEaVv/tjDPkntTqFSeftN3N0Fa\nwYBz9yU1K1mcweE64G/t3DcQCBha15RXtFtloWW5f5vPonf3NXz4GMz5B6ycB3vd53fdFAzGevLH\nZ1vjdsZFYvvo7XMa101tvzhvvfvZfd+FSUiWbUO7dFKkQBj3Zf3nwx2MrZRFn+u7G0fvCnv3WfRp\nijPYz02fBcyMituAo0SkQUTGAROApzve7UBgHWBqX3j08+nbz/oZzEio8uXz0dvYNwGTrXF5VNAj\nlevGE2uv2nRdVsi36LNrtTWf9xQRuW5q0lj0HtdNpqbQLZIb2Izap8mH5LpuKpWnP1eT13lC6CIf\nfUmLPmVxhu+JyD5AC7AMOC7a92URmQq8ArQC31WqkhUOAoFeinGzLPhv6bav/xWWvw7ziqTGN0Kf\nqfcX5/YVI1m9QL+W66M3fPi4/gMrnXT02roivyB5NhJ623VTMJmohFzVD4ufBvQBirf3USD0FXLd\nGIs+F+UUPSmYnD41jf7vsEKkmjCVojjDD4vsezZwdns7GAisM9h5WtxMh8WY8T39WmyKvkmBUNMn\nQejtyVTOdt+EqTQWvSHTEPueh++iX9+bBqOsaTVpLHpfymKbhqFgBd3EFn0Z7pfOGoztP1a/mlz2\n5jwtUYfrBnWq0IcUCIF1mhSTAceIyIMiMlNEXhCRA6P19SLyz2ii4PMismeHO2Nbj6aYRjm4g602\nbdGEKftmsMvVsMs1UaoC26K3hP7l3+WHLBqM0C96EFbOLy5StstoxCf1QOiCWx3XzZpCi75A6M0T\nQIILxo2JL3CDpHDdmH3MxKZKWfSbfhN2uxG2Pg22PBX2vFOvb7WEvhMJQh9YZ0k5GfBXwFSl1Lbo\n8amLo/XfAFBKbQ18BviDVUS8fdiC+NjRHdvfxQzG2kI/7lgY9yUtbj6LXmrgramwxoqfMCkHjNDf\nvxfcPqG4RW8fO1MLg7eBVW/6ffSZBmsQtUzXTY0r9K5Fn0LojWslJ7yVirrJwJgj9Ovk3+k5BRCE\nPhDoAkpOBkSrhImJGwQYU3tL4AEApdQHwEfAlA71xhbqlXM7dKgCslFSM697J+MX+v7j9fuVb1pN\n67Xgtq6KXU2qNbbokwpp552uTkfJFAh9iz5+LnlZgtAnDaoWWPTtkLesUye2UhZ9EmawOs331gGC\n0AfWZdJM6DsdOFZEFqDHqb4frX8eOFhEaqOIsu3JDyUGypwM6I1T6EDlrPohcFh0X8pZ9J7aulJD\nnuWaq5XaX79fNV/nqTH9qe2rhd624s37NJap1OoZvB8+mb9/tjm+kUC6FAg2BbnlI3kbfZguQD/x\n1NJ9M6GfJoKnVNnEjmIs+o7k0UlBEPpAoDhHA1cqpUYDBwJXRy6aK9A3hhnARcDjQIFSlzUZ0Otj\n74DQ9Bsb5YCvi330GY9FLzX5xcFtoW9bA6vehgFRdSYRLahtq/Jj9nMWfQrBytTqIiUPH5y/vxH6\nJIu+1GBsko++YSgc8Gy6usKmGlSu8lMnW/Tb/A76jdO1aKHTSmIGoQ+sy6SZ0Pd1YCqAUuoJoBEY\nrpRqVUr9WCk1WSl1CDAYeL1DvSnmY28PdvbGV86Bj1/Kz9+e1y7Bol/zru6XXYavpp+26POEfo0W\nVl/FpoLzeQS7rUlb0zWWRV9OHD1UxnUz+lD41G16wBQ633Uzcg84ZF5hnvoKU3VC39QEP/kJLFlS\num0gUIKSkwGBt4G9AURkC7TQLxaRviLSL1r/GaBVKfVKh3rjum6G7tChw8XZGy0rPsmi9/no7aIc\nuVwykUXfukrHwhtaV0W5bNzwRF+/fEK/JjpGv9hyL3swtm/+cnuEXgRGHxTfNDpb6HPn7VwprrrC\nI9dfDxdcAM3N8Kc/dXdvAtVMysmAJwOXisiP0X6U45VSSkTWA+4VkSz6KSBhTn0ZuK4bn/VdFp6k\nXkkWvZvrBvIzO9YPtY7RT8fW2xb9moXa/+8T+t1vcrqVYNGbtMGS5LopMWu0ZHhlGeTSEweh71Jm\nzoSzzoKbb9bLa9cWbx8IpCHFZMBXgE969psPbO6u71hnHIveN3BaDkY87NTFSYOxvjh6W+hNVIjx\n0besyBf61Qv8Fv3GR8FGhzvn88hOy3LtIqrrn2zRqxJhkpVw3Rhq+mo3zmYJKSWqjKpx3cybF4s8\nQFv0m1i6FJ55pnv6FAhUFNdH74qyEboVc+C+XaH5I4pihM6e2epmZ4Rk141P6EkYjP3wCd1ft1CH\nz8L3WfSmYEhNv+TB2FIkRd20BxH41C26jmwvoGqE/tBD4YYb4uW339av++wDO+4IrRUexwoEuhzX\ndePGvBur+8XfamFdeHvx4xnXhZ190ieySTNj7Vmqee89g7EAK94oFHaf0PsE3Ah9Xf/kwdhSVNKi\n72VUzTdRU6NF3TBnDsyYoV06AAtD8uNAtWNb1VKTnEnRiHVS7vncMYzrxhJxn3j6LPpMfb71b3z0\now+zhD4ajN33Cf9ngOQnCBefRV9wUzLl99qbAmHdpWqEHmDwYPj972HrrbUrZwcrKGH+/G7rViBQ\nGWzXjdQWThAyImoGK7NrYeZPihzQ8/OWGpj4K9h7en4716LP1OffaOqHwKELYYe/5odXSi0M2wn2\nfQo+8xg0faDb99kgOnQFLXrzhGMSg7kEiz6RqhmMNfz0p7DllnDiifDee/H6PfeEp5/OF/9AoKqw\nhT5Tm5/GFwot+kUPwdvXJx/PJ3RSC9uc6axLsOjtIhw1jfFs0ZxF/7H23YvA8KgU9NpI6PuN1TH4\nvuLaBZa6xJ+9tohFP3hr2Oky/VThoyC9QzUKfZgwleNzn4N334UvfSl//Y47wg8TEyYHAj0ce3aq\n1HpcN63xNns5CZ/rwheimEly3VjnzwvR7Acobb3XDcg/1poovXK/jaP9Ulj0tiVea1v0npQHm3xd\nz3T14bqJgkWfo6q/ie98p3Ddn/8MH3/c9X0JBDqMa9Enum5q85dtPvMoDPxE1C7Boi/AMzPWFXrb\nMjcDs6sX5MfX258h57rxiLVrqecJfTEffQlciz746HNUtdDvuquOOHOt+Kuvhksugd/8Ri/fcgvc\ndFPh/oFAj6JgMLaE68Y3a3PgFtYs1gQfvW+dO2HKFXr7pmGEefVbOre8zT4PweRzrRDJFDcb16Jf\nE/lkG0cW7luMgsLbVS1vFaXqfPQ+LrpI++unTtXL3/9+vO3gg+HwaL5GZyeiCwQ6RJ4rJtM+i762\nbyzmRugOngtPfQMWPZAgvM5grGrR/vmkdAYm7/ua92C9PfK3Dd1O/5lBYm+Ujyv0Vrx+bb+4Vu1G\nR/jPn0TjejpPjUmWFoQ+R6/5JowbZ8cd89dPsTKEB6EP9GjsOHqRQh918zJ9EefCKz0lATMNscAZ\nwe8/HkbuVXiO3LlS+OhtbAu8Ybi/jblxeMcJSlj0478GfTaEAZv4j12M0QfZJyp//15Kr/km9thD\n/waeegoWLdKum732ym9z7LGw0lMVLRDoEeRZ6JIf9QJw50S4e5vYIvZVdRIh97O2LVrzdKA8sfeu\nRe9z3dikEnrz9OFzFSUIvWT0OXe+HA59p3C/cgk++hyphD5FXc2TROSVqKbm/SKysbWtTURmRX9u\nZsBOYb314BvfgPvvz19/7bVw3XVd0YNAoB3kuW48Fj3ARy/GAuYr2g2WRe8Ret8kq06x6IsIfYFF\nH7lupDaeDFWJvOxV5brpXHdDyW8iZV3NmcAUpdQk4EbgPGvbmihn92Sl1MF0M3PmxO+feQbef7/7\n+hII5OG6bpIqKhmxTqrTmhM46+ctPUjoEy36Cg8ZVpXQdy5pvomSdTWVUg8qpYx58SS6gEOP4CfO\nxMF//Qv23VfnztlxRz3xKhDoEbiumyShNwLflmDR4/jooYRFnzAz1nUdGezB0yShNzetNBa9Sd42\nZBv/sdpNEHpDmm8iTV1Nm68Dd1vLjVHNzCdF5FDfDmXV1SyTc8+FFSvgwQfh6KO1BT9tGvz3v3r7\n3XfDiy/qnDlLl8LDD1f09IFAelzXTZJFbcr2tazwby/muvH66Mu06PuNid+74ZWGciz61ZG8jNrf\nf6z2Eiz6HBX9JkTkWGAKcL61emOl1BTgGOAiESkYSi+rrmaZZDLQv79OkfDHP8Lf/w5fjkpE/PrX\nOuvlpEmw3Xaw9956UDcM2Aa6hYKEYEmum0joW9sh9N5EaCkmTLnH3/7P+n2/jf1tyhH6Tb+pc79v\nXuFp7Z1Uf7UaSSP0aepqIiL7AL8EDlZK5cqCKKUWRq/zgOnAth3ob4cYORK++U248kp4+WX4+c91\nVkzDrFn69dVX9aSrww/3HiYQqDwr5sLjVk4PX3iloS36eSVlr/QJfSV99ACbfx+OatXJznyUE145\ncHOd+70+RWHxQLtII/Ql62qKyLbAP9Ai/4G1foiINETvh6Mr9XSsrmYFyGR0YrS+fWGQ59qaPRu+\n8hU9o3bu3K7vX6DrSBFRNkZEHhSRmVFU2YHR+joR+ZeIvCgis0Xk1A515JVz3TMnzy41rptEyvXR\neypMFZswlTtmkfDFciz6EAbZ6ZQUeqVUK2Dqas4Gppq6miJiomjOB/oDNzhhlFsAM0TkeeBB4NwO\nF1CuMOutp1+/8Y143c+tn/u0aV3bn0DXkTKi7Ffoa35btJFzcbT+SKBBKbU1sD3wTREZ2/7OuD9F\nide5gpstIfS+qBtzDN9sWrdmbOtqyKQs9J1E3yjXjc+HX1D0Owh9nLWyc9xNqeKZUtTV9NbbUko9\nDmzdkQ52Ntdfr4uMX3yxjsDZbTedGdNw772wyy6wTaUDAgI9gVxEGYCImIgy2xhRgEmiMgh411rf\nT0RqgT5AM+CUXCoH5wcuQlxoow6wxL2URe9z3YzaDzb5Bmx9mqd95LppXQMLb4O1i3VSso4I/aSz\nYMh2+rwF5wtCX8C442DJMzDpzNJt28E6Pyw9aRJcfjnU1up0CSYaB2DIELj1Vpg8GTbZRA/mBnoV\naSLKTgeOFZEFaGPHZFK6EVgFvAe8DVyglFra/q64lpzE7hR3UDa168b6edfUw06XQF9P5LMJr3zh\n1/DYUXpdvzEdE/qaBhh7tH9ANAh9IbV99IzgxoRw1Y4evlOOWsXst59Oc3zppXqg9sc/1uvnzYOT\nToI+fWDxYh25M3Zst3Y10DUcDVyplPqDiOwCXC0iW6GfBtqADYAhwCMi8j/zdGAQkROBEwHGjBlD\nIj7XTTlC3zgStvtj/rHSCqix6FcviNd1VOiLUeC6WeftzU4nCL2HgQPh5JN1xSqXb39bvy5erHPf\nAzzwgG7784KhvEAPJ01E2deB/QGUUk+ISCMwHB0ufI9SqgX4QEQeQ4cW5wm9UuoS4BKAKVOmFJnn\n7nHdlCP0u14L60fJnXw++mKYwVi7iEjfMcUHWztCsOi7nHArLcKUKXDeeXpC1bx5YBtkL70EzVHy\nwD/9CX7xC/jgA/9xAj2WkhFlaLfM3gAi/9/euUdJUV17+NszvIYZrzx0qQslgGBQEBmYIKKSEBVE\nk8FHDDM+LhqM0agxGJ/BFZC4XBGNS43eeNGLV41K1MRHJD6QBMXgA0RAUFEC+OCigETXCiII7PvH\nrrKrm56ZHpjuqu7Z31q9qurUqe7dPWd+dWqfc/aWg4EOwPqg/LtBeSUwFHhnly3ZsSWjINNHH62b\nRejTbgZZXDeNEQ7Gbv1Xqix08VT1hpo7cnufXMkUdhf6vONC3whlZRZCoX9/6NnT5t+PGgV1dbbS\ntrLSFlctXGiRM59+usm3dBJEjjPKfgH8OJg59hBwtqoqNlunSkSWYTeMe1R1yS4bsy1jlZ5IKoFI\n2FMP2Z55UyC9l5xtMLYxQtfN5kgS5jCjVO17cFCWVG67Q75n3XSObalOYnHXTTMYMcJeU6daFMxt\n2+Ccc+CjwLU5cyaMG2f7r74Kc+fa/mWXxWOv0zQ5zCh7C1v/kXndv7Epli3DV5nLsQW61kDtKutp\n//N/UqeyuW6iPfrm+ugJBmM3r7WZMtVTm75kd8i36+a4lxoOD9FKcaHfBXr1Su0/+qhtDzwQHnnE\nhH7YMDj//FQdF3qnSbZvyigIfPZVPeCzDNHKKvQt0KP/8mPofirse0xu1+0q+Rb6Nh1TeW0dwIV+\nlzjlFHjqKYuLM2+euW569YKxY+G+++wVZdkyW4nroTecBsns0UcbS6YQZl0wVZZlP0ehLyuHTats\nv8O+uV2zO/iCqYLjPvpdoKwMTjzRgqWNHGmzbU480UIf33WXRcm8/XY4+GCr378/XHNNvDY7CSfT\nRx8lUwizzqOPTOhpbo8+lIGKbnBAAQI87dSjdxnKN96jbyEqKy02DsC559q2UydLXwhw/fVw0UUW\nMO3oo2HNGpvF4718B1Vzm6TRSI8+63tEYtU0ex598Fk9zoCqnrldszt4j77g+K00j3TLWGN53HGW\nx7ZjR1tsVV0NV+9eKCynFPjbMelTG4F0oc+hP5aZtARy7ylvDm4yVb0ar9dS+Dz6guNCn0f2j6w2\nHzbMfPUAXwUBBBcvtsQoixZZp85ppXQaYNuOB8DewQSfxnz0mXQ/rYHsTDn+e29637aFEnpfGVtw\n/BfOI2GP/qCDbP49pIt/SHW1Lbpau3bnc04roPsPbdt5IFTfHBQ2IfTRsqMezlgwFS60ytEvuGm1\nbePq0Tt5x4U+j1RUWJC0OXPMZQMm9A8+uHPdCRMseBrA6NEWNnnWLLjkEpvVs2ULfNlULCunONnr\nCBh4Awy6ORJ2oAmhb5tLko4chT4c3K1sJBZPS+JCX3Bc6PNMbS3stx8MHQo//WlqVs6gQXa+vh7C\n7Inr1lnP/pln4O67zX9/220WiqFDB+jXDx5+GI49FnbsaPgznSJDBA65AvboTcq/HhHpTFcHpMel\n2V1GzgtuMg1ktGppsn0fJ6+40BeINm3gjjtsqiVYOIWzzoJ77zVf/ZQpVv7zn6euef11uzmErFwJ\n114Ls2d75qvSJRysaapH/x87l+0qXb8FfSe03Ps1RbRHP+C6wn1uK8aFPiYOPdQWVrVtaz3+a66B\nG27Yud4pp6Tnrn0rSImxcGGq7JNPfDC3ZNBchb4R103SG0PYo5dy6D8xXltaCS70CUEErrgCFiyA\nO++E1ath0iQYPhzuuQduusnqdewI7dqlhP6jj2DffS3HrQ/mlgJZBlJ3tUef1EUa7qMvOP6LJ4zB\ng+0FMHmybdu2hZ/9zI7r6mDJEgusNnu2iT7AH/4AL71kLp2yMvjiC/Prl/mtvLhoiR590vF58wXH\nZaBIaNsW5s+3dIa33GIrcRcuhJdftvPV1fYUcPzx5suvrEzdKJxiIpvQZ+mPlVcUxJq8EA769jon\nXjtaETkJvYgcLyLLRWSFiOyUR0lELhWRt0RkiYjMFpFvRM6NE5H3gte4ljS+tdG3r8XXOeIIm6Gz\nMchQ2rs3vPCC7c+aZZE0AX6dkWd440a7YTyZmVrDSR5prpss/6aNzlxJuI9eyuC0z+Fbd8ZtSauh\nSaEXkXIsycJo4BCgXkQOyaj2BlCjqgOwpMlTg2u7AJOAw7Ecm5NEpHPLmd966djRYuksXWpJUPbY\nw2Lgf+c76fWWLrXkKKq2MnfbNhgzJn0w10kQXWqg9/kw7IFUWSj6UXdNTn7uhProwcYY8pWq0NmJ\nXHr0Q4AVqrpSVbcCM4Ax0Qqq+ndV/SI4fAXLvQkwCpilqhtV9V/ALIL8m07L0K9farXtkCHw+OPW\nYw+ToRx6qN0EamrSI2gOHgzTp6eON2wonM1OI5SVw5DfB3PqIxz1KIxelDoOhb5NC86nd0qWXIS+\nG/Bh5PijoKwhxgNhUr2crhWR80RkgYgsWL9+fQ4mOQ2x557w/e9b+IWf/MTKwh78iy+m173/fts+\n9JAt2po/v7C2Os2g+6mWhCQambJ2FdSubPQyx4EWHowVkTOBGuDG5lynqtNUtUZVa/YOl4k6u82t\nt9osnMcfT0+Gsnw5XHopvPKKpUI8/XQrD/38Idu3W0///ffhsMNg1arUuU2bLCxDsZPD+FN3Efm7\niLwRjEGdEJSfISKLIq8dIjIw/wYHPfmyNib8HfbKUinhPnqn4OQi9GuAAyLH+wdlaYjIscBEoFZV\ntzTnWic/tG+fSns4YECq/KCDLDvWl1/aCt2QOXPgBz+wdIg33gjf/Ka5hS680KZ0Rut26mRhl4uZ\nHMefrsGShlf+mBZXAAAKDElEQVQDdcB/AajqA6o6UFUHAmcBq1R1EfkmFPpi99E7BSWX1jIf6CMi\nPTGRrgNOj1YQkWrgv4HjVXVd5NSzwPWRAdiRgEdgj4G+fdOPR42yBVpgSVHq6y3nbTZmzrRtOMX7\ngw9sUHfuXCtL6rqcHPh6/AlARMLxp7cidRQIVyftCfxflvepx8au8k9ZG9iOx4txmkWTrUVVt4nI\nRZholwPTVXWZiEwBFqjqk5irpgp4ROy//gNVrVXVjSLya+xmATBFVTfm5Zs4jdK+vaU9HD48dRwN\nuXDBBfDhh3DCCTZ9M+yt33RTKrn5U0/Zgq1w7j6Ya6h3b9i82aJ1FhnZxpAOz6gzGXhORC4GKoFj\ns7zPWDImKISIyHnAeQDdu7dEdMhmZo9yHHJcGauqfwX+mlH2q8h+tsYfnpsOTG/ovFM4nn224XMj\nRqQEPAyHXF5uvnwRm5P/xhs2yyeMvAlwxhkWYnncOHPt1Nenv++2bfZ+VVUt+lUKST3wv6r6WxE5\nArhfRPqrWu4+ETkc+EJVl2a7WFWnAdMAampqdt95Hj4+Nea6SXqsG6fg+MpYZyc6dLCbwtKlpiuX\nXgpdu6bOL1wIp54KEyfCa6+ZuG/dChdfDM8/n/5el11m0zs/+8yeGBJGLmNI44GHAVT1ZaADEB0B\nrQMeyqONGQRCn4vrpoh9ak7L4kLvZGXkyHS//nXX2ZTNoUPt+PDD0/Pd/upXtuq2vt7Ef+ZMS5l4\n6612vnNnS4YenbkDsXc+vx5/EpF2mGhnrhv+ADgGQEQOxoR+fXBcBvyQQvnnIZUE3AODOc3Ahd7J\nibo6W4R17rl2fOSRFk8n5PLLLbb+hg12E/je91IB16LMm5fanzvXgq6FuXQLjapuA8Lxp7ex2TXL\nRGSKiNQG1X4B/FhEFmM997NVv749DQc+DAdzC8JXn9m2Y5aclI7TAN4tcJrFj35k/vxw2uZLL1kw\ntaoqy3w1dqz13Bcvhueeszpjx8If/5iq/847Frbh00+t7JlnzPcPNr2zf38L0XzmmXDSSXDaafn7\nPjmMP70FHNnAtXOAofmzrhGqDmzkpPvonXRc6J1mIZISebCe/ZGBDJaVwYyIE2PzZhP7AQNSQn9n\nljhW774Ln39ukTn/9Cdb4NW/v+XWffBBi9E/eDC8+aa5hcaPz9/3Kxr2aEzoHScdd904eaOiwsIv\n9OxpgdXC5Clgoj15svnup02zBVjXXmvnRNJ78TU1Fnt/xAhzHa1bh9OusdiAPgjrpONC7xSEyko4\n+eTUceiemZAlVenUqTuXXXJJytVz3HHw8ccNf9aUKbAo/2tUHadocKF3CkavXpb8/IknUmVXXQUr\nVsCwYeb2efddq/Pppybm3/42XHll+oDtkiXwy19Cba25ewB27IBHHzXf/6RJO8ftKRmGPQhHP9ZE\nJffRO+m4j94pKKEwh7Rta4lS/vGP9PIuXWw7Z475+sNVvBMmwN13Wx5dgL/8xQR+woSUu6eiAs4+\nO1/fIGZ61Ddd52vcheMYLvRO4qmogLffht/8xnrrfftaCOabb7bB2RkzUtM2Tz4ZRo+2cM2O4xgu\n9E5R0LdvKnrm+PE20yeckqlqs3q6dYM//zk2Ex0nsbiP3ik6ystTIg8WogEsUJsDlAUr1cIk3E6r\nx3v0TtEzZIiFWj7mmLgtSQj9roYdW6HPBXFb4iQEF3qnJLjasxykaFMJ1VnmqDqtFnfdOI7jlDgu\n9I7jOCWOC73jOE6J40LvOI5T4rjQO47jlDgu9I7jOCWOC73jOE6J40LvOI5T4ojGnJ05ExFZD7zf\nwOm9gA0FNCcX3KamSZo931DVvQv9od62d5uk2QPJsqnBdp04oW8MEVmgqjVx2xHFbWqapNmTRJL4\nGyXNpqTZA8m0KRvuunEcxylxXOgdx3FKnGIT+mlxG5AFt6lpkmZPEknib5Q0m5JmDyTTpp0oKh+9\n4ziO03yKrUfvOI7jNBMXesdxnBKnaIReRI4XkeUiskJErorJhtUi8qaILBKRBUFZFxGZJSLvBdvO\nebZhuoisE5GlkbKsNohxW/CbLRGRQQW0abKIrAl+q0UickLk3NWBTctFZFQ+bCoWktCuAzu8bedm\nT3G2a1VN/AsoB/4J9ALaAYuBQ2KwYzWwV0bZVOCqYP8q4IY82zAcGAQsbcoG4ATgaUCAocCrBbRp\nMnBZlrqHBH+/9kDP4O9aHncbi+OVlHYd2OJtOzd7irJdF0uPfgiwQlVXqupWYAYwJmabQsYA9wb7\n9wIn5fPDVPVFYGOONowB7lPjFaCTiOxXIJsaYgwwQ1W3qOoqYAX2922NJLldQytv26XUrotF6LsB\nH0aOPwrKCo0Cz4nI6yJyXlC2j6quDfY/BvaJwa6GbIj7d7soeKyeHnnsj9umJJGk38Lbdu4UXbsu\nFqFPCkep6iBgNHChiAyPnlR7hot1vmoSbAj4PXAgMBBYC/w2XnOcJvC2nRtF2a6LRejXAAdEjvcP\nygqKqq4JtuuAx7BHs0/CR8Zgu67QdjViQ2y/m6p+oqrbVXUHcBepx9hE/C0TQmJ+C2/buVGs7bpY\nhH4+0EdEeopIO6AOeLKQBohIpYjsEe4DI4GlgR3jgmrjgCcKaVdAQzY8CfxnMENhKPB55DE4r2T4\nS0/GfqvQpjoRaS8iPYE+wGuFsCmBxN6uwdt2cyjadh33aHCuL2yU/V1sNHtiDJ/fCxtVXwwsC20A\nugKzgfeA54EuebbjIeyR8SvMDzi+IRuwGQl3BL/Zm0BNAW26P/jMJdg/wX6R+hMDm5YDo+NuW3G+\n4m7XgQ3etnO3pyjbtYdAcBzHKXGKxXXjOI7j7CIu9I7jOCWOC73jOE6J40LvOI5T4rjQO47jlDgu\n9AlBRLZHIuItaslIhiLSIxqBz3EKibft+GkTtwHO12xW1YFxG+E4ecDbdsx4jz7hBHHCpwaxwl8T\nkd5BeQ8R+VsQXGm2iHQPyvcRkcdEZHHwGha8VbmI3CUiy0TkORGpiO1LOQ7etguJC31yqMh4vB0b\nOfe5qh4K3A7cEpT9DrhXVQcADwC3BeW3AS+o6mFYLO1lQXkf4A5V7Qd8Bpya5+/jOCHetmPGV8Ym\nBBH5t6pWZSlfDXxXVVeKSFvgY1XtKiIbsOXXXwXla1V1LxFZD+yvqlsi79EDmKWqfYLjK4G2qnpd\n/r+Z09rxth0/3qMvDrSB/eawJbK/HR+fcZKBt+0C4EJfHIyNbF8O9udh0Q4BzgDmBvuzgQsARKRc\nRPYslJGOswt42y4AfudLDhUisihy/IyqhtPQOovIEqznUh+UXQzcIyKXA+uBc4LyS4BpIjIe691c\ngEXgc5y48LYdM+6jTziBH7NGVTfEbYvjtCTetguHu24cx3FKHO/RO47jlDjeo3ccxylxXOgdx3FK\nHBd6x3GcEseF3nEcp8RxoXccxylx/h+Fdj6Sy9CVnwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXJjXT1V9Xq1"
   },
   "source": [
    "## 8.0 Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZcWydmIVhZGr",
    "outputId": "239b8549-253f-4d6b-fece-b235dc67d04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 319us/sample - loss: 0.3540 - acc: 0.9047\n",
      "Test loss: 0.3539511471837759\n",
      "Test accuracy: 0.9047\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_on_CIFAR_10_using_Keras_epoch107.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
